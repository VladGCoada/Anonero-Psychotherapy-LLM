{
  "best_global_step": 5500,
  "best_metric": 1.7588139772415161,
  "best_model_checkpoint": "./mistral-books-finetuned\\checkpoint-5500",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 5697,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005266622778143515,
      "grad_norm": 0.6212164163589478,
      "learning_rate": 1.8e-05,
      "loss": 1.914,
      "step": 10
    },
    {
      "epoch": 0.01053324555628703,
      "grad_norm": 0.6152288317680359,
      "learning_rate": 3.8e-05,
      "loss": 1.8831,
      "step": 20
    },
    {
      "epoch": 0.015799868334430547,
      "grad_norm": 3.1747231483459473,
      "learning_rate": 5.8e-05,
      "loss": 1.8555,
      "step": 30
    },
    {
      "epoch": 0.02106649111257406,
      "grad_norm": 0.7466143369674683,
      "learning_rate": 7.800000000000001e-05,
      "loss": 1.8343,
      "step": 40
    },
    {
      "epoch": 0.026333113890717578,
      "grad_norm": 0.6748694777488708,
      "learning_rate": 9.8e-05,
      "loss": 1.8348,
      "step": 50
    },
    {
      "epoch": 0.031599736668861095,
      "grad_norm": 0.8626570105552673,
      "learning_rate": 0.000118,
      "loss": 1.9028,
      "step": 60
    },
    {
      "epoch": 0.03686635944700461,
      "grad_norm": 0.805627167224884,
      "learning_rate": 0.000138,
      "loss": 1.916,
      "step": 70
    },
    {
      "epoch": 0.04213298222514812,
      "grad_norm": 0.6627111434936523,
      "learning_rate": 0.00015800000000000002,
      "loss": 1.8456,
      "step": 80
    },
    {
      "epoch": 0.04739960500329164,
      "grad_norm": 0.540079653263092,
      "learning_rate": 0.00017800000000000002,
      "loss": 1.8719,
      "step": 90
    },
    {
      "epoch": 0.052666227781435156,
      "grad_norm": 0.5380985140800476,
      "learning_rate": 0.00019800000000000002,
      "loss": 1.8666,
      "step": 100
    },
    {
      "epoch": 0.05793285055957867,
      "grad_norm": 0.5923865437507629,
      "learning_rate": 0.00019967839914239772,
      "loss": 1.8126,
      "step": 110
    },
    {
      "epoch": 0.06319947333772219,
      "grad_norm": 0.6007402539253235,
      "learning_rate": 0.00019932106485617297,
      "loss": 1.8331,
      "step": 120
    },
    {
      "epoch": 0.0684660961158657,
      "grad_norm": 0.5724003911018372,
      "learning_rate": 0.0001989637305699482,
      "loss": 1.7969,
      "step": 130
    },
    {
      "epoch": 0.07373271889400922,
      "grad_norm": 0.5611459016799927,
      "learning_rate": 0.00019860639628372344,
      "loss": 1.8975,
      "step": 140
    },
    {
      "epoch": 0.07899934167215274,
      "grad_norm": 0.5062956213951111,
      "learning_rate": 0.00019824906199749867,
      "loss": 1.9507,
      "step": 150
    },
    {
      "epoch": 0.08426596445029624,
      "grad_norm": 0.5786814093589783,
      "learning_rate": 0.00019789172771127392,
      "loss": 1.8065,
      "step": 160
    },
    {
      "epoch": 0.08953258722843976,
      "grad_norm": 0.5402042865753174,
      "learning_rate": 0.00019753439342504913,
      "loss": 1.8292,
      "step": 170
    },
    {
      "epoch": 0.09479921000658328,
      "grad_norm": 0.5433791279792786,
      "learning_rate": 0.00019717705913882438,
      "loss": 1.8301,
      "step": 180
    },
    {
      "epoch": 0.10006583278472679,
      "grad_norm": 0.5347517132759094,
      "learning_rate": 0.00019681972485259961,
      "loss": 1.8097,
      "step": 190
    },
    {
      "epoch": 0.10533245556287031,
      "grad_norm": 0.6489611864089966,
      "learning_rate": 0.00019646239056637485,
      "loss": 1.8275,
      "step": 200
    },
    {
      "epoch": 0.11059907834101383,
      "grad_norm": 0.47770950198173523,
      "learning_rate": 0.00019610505628015008,
      "loss": 1.835,
      "step": 210
    },
    {
      "epoch": 0.11586570111915734,
      "grad_norm": 0.5557510256767273,
      "learning_rate": 0.00019574772199392533,
      "loss": 1.9178,
      "step": 220
    },
    {
      "epoch": 0.12113232389730086,
      "grad_norm": 0.4887083172798157,
      "learning_rate": 0.00019539038770770056,
      "loss": 1.7786,
      "step": 230
    },
    {
      "epoch": 0.12639894667544438,
      "grad_norm": 0.536496639251709,
      "learning_rate": 0.0001950330534214758,
      "loss": 1.8731,
      "step": 240
    },
    {
      "epoch": 0.1316655694535879,
      "grad_norm": 0.517756462097168,
      "learning_rate": 0.00019467571913525105,
      "loss": 1.8048,
      "step": 250
    },
    {
      "epoch": 0.1369321922317314,
      "grad_norm": 0.5753263235092163,
      "learning_rate": 0.00019431838484902628,
      "loss": 1.8163,
      "step": 260
    },
    {
      "epoch": 0.1421988150098749,
      "grad_norm": 0.574120283126831,
      "learning_rate": 0.0001939610505628015,
      "loss": 1.7685,
      "step": 270
    },
    {
      "epoch": 0.14746543778801843,
      "grad_norm": 0.5592824816703796,
      "learning_rate": 0.00019360371627657674,
      "loss": 1.8342,
      "step": 280
    },
    {
      "epoch": 0.15273206056616195,
      "grad_norm": 0.5639145374298096,
      "learning_rate": 0.000193246381990352,
      "loss": 1.8247,
      "step": 290
    },
    {
      "epoch": 0.15799868334430547,
      "grad_norm": 0.5851631760597229,
      "learning_rate": 0.0001928890477041272,
      "loss": 1.9287,
      "step": 300
    },
    {
      "epoch": 0.16326530612244897,
      "grad_norm": 0.5412255525588989,
      "learning_rate": 0.00019253171341790246,
      "loss": 1.7723,
      "step": 310
    },
    {
      "epoch": 0.1685319289005925,
      "grad_norm": 0.5012082457542419,
      "learning_rate": 0.0001921743791316777,
      "loss": 1.7978,
      "step": 320
    },
    {
      "epoch": 0.173798551678736,
      "grad_norm": 0.627496600151062,
      "learning_rate": 0.00019181704484545292,
      "loss": 1.8888,
      "step": 330
    },
    {
      "epoch": 0.17906517445687953,
      "grad_norm": 0.6324867010116577,
      "learning_rate": 0.00019145971055922818,
      "loss": 1.8279,
      "step": 340
    },
    {
      "epoch": 0.18433179723502305,
      "grad_norm": 0.6434245109558105,
      "learning_rate": 0.0001911023762730034,
      "loss": 1.7824,
      "step": 350
    },
    {
      "epoch": 0.18959842001316657,
      "grad_norm": 0.522665798664093,
      "learning_rate": 0.00019074504198677864,
      "loss": 1.814,
      "step": 360
    },
    {
      "epoch": 0.19486504279131006,
      "grad_norm": 0.5316786170005798,
      "learning_rate": 0.00019038770770055387,
      "loss": 1.7873,
      "step": 370
    },
    {
      "epoch": 0.20013166556945358,
      "grad_norm": 0.5427834391593933,
      "learning_rate": 0.00019003037341432913,
      "loss": 1.7142,
      "step": 380
    },
    {
      "epoch": 0.2053982883475971,
      "grad_norm": 0.5471122860908508,
      "learning_rate": 0.00018967303912810433,
      "loss": 1.838,
      "step": 390
    },
    {
      "epoch": 0.21066491112574062,
      "grad_norm": 0.5662885904312134,
      "learning_rate": 0.0001893157048418796,
      "loss": 1.839,
      "step": 400
    },
    {
      "epoch": 0.21593153390388414,
      "grad_norm": 0.5427488684654236,
      "learning_rate": 0.00018895837055565482,
      "loss": 1.8466,
      "step": 410
    },
    {
      "epoch": 0.22119815668202766,
      "grad_norm": 0.5619803667068481,
      "learning_rate": 0.00018860103626943005,
      "loss": 1.8282,
      "step": 420
    },
    {
      "epoch": 0.22646477946017116,
      "grad_norm": 0.5845116376876831,
      "learning_rate": 0.00018824370198320528,
      "loss": 1.7421,
      "step": 430
    },
    {
      "epoch": 0.23173140223831468,
      "grad_norm": 0.5333179831504822,
      "learning_rate": 0.00018788636769698054,
      "loss": 1.8345,
      "step": 440
    },
    {
      "epoch": 0.2369980250164582,
      "grad_norm": 0.538374662399292,
      "learning_rate": 0.00018752903341075577,
      "loss": 1.7955,
      "step": 450
    },
    {
      "epoch": 0.24226464779460172,
      "grad_norm": 0.5759226083755493,
      "learning_rate": 0.000187171699124531,
      "loss": 1.7878,
      "step": 460
    },
    {
      "epoch": 0.24753127057274524,
      "grad_norm": 0.5964107513427734,
      "learning_rate": 0.00018681436483830626,
      "loss": 1.8678,
      "step": 470
    },
    {
      "epoch": 0.25279789335088876,
      "grad_norm": 0.5315794348716736,
      "learning_rate": 0.0001864570305520815,
      "loss": 1.8639,
      "step": 480
    },
    {
      "epoch": 0.25806451612903225,
      "grad_norm": 0.5170924663543701,
      "learning_rate": 0.00018609969626585672,
      "loss": 1.8139,
      "step": 490
    },
    {
      "epoch": 0.2633311389071758,
      "grad_norm": 0.6019985675811768,
      "learning_rate": 0.00018574236197963195,
      "loss": 1.8834,
      "step": 500
    },
    {
      "epoch": 0.2633311389071758,
      "eval_loss": 1.8224092721939087,
      "eval_runtime": 2220.9877,
      "eval_samples_per_second": 0.76,
      "eval_steps_per_second": 0.76,
      "step": 500
    },
    {
      "epoch": 0.2685977616853193,
      "grad_norm": 0.5755082368850708,
      "learning_rate": 0.0001853850276934072,
      "loss": 1.781,
      "step": 510
    },
    {
      "epoch": 0.2738643844634628,
      "grad_norm": 0.6202906966209412,
      "learning_rate": 0.0001850276934071824,
      "loss": 1.8383,
      "step": 520
    },
    {
      "epoch": 0.27913100724160633,
      "grad_norm": 0.5534796118736267,
      "learning_rate": 0.00018467035912095767,
      "loss": 1.8952,
      "step": 530
    },
    {
      "epoch": 0.2843976300197498,
      "grad_norm": 0.6289898157119751,
      "learning_rate": 0.0001843130248347329,
      "loss": 1.8188,
      "step": 540
    },
    {
      "epoch": 0.2896642527978934,
      "grad_norm": 0.5336708426475525,
      "learning_rate": 0.00018395569054850813,
      "loss": 1.8271,
      "step": 550
    },
    {
      "epoch": 0.29493087557603687,
      "grad_norm": 0.5846602916717529,
      "learning_rate": 0.00018359835626228336,
      "loss": 1.7969,
      "step": 560
    },
    {
      "epoch": 0.30019749835418036,
      "grad_norm": 0.604201078414917,
      "learning_rate": 0.00018324102197605862,
      "loss": 1.807,
      "step": 570
    },
    {
      "epoch": 0.3054641211323239,
      "grad_norm": 0.5600471496582031,
      "learning_rate": 0.00018288368768983385,
      "loss": 1.7511,
      "step": 580
    },
    {
      "epoch": 0.3107307439104674,
      "grad_norm": 1.0727484226226807,
      "learning_rate": 0.00018252635340360908,
      "loss": 1.7943,
      "step": 590
    },
    {
      "epoch": 0.31599736668861095,
      "grad_norm": 0.5548029541969299,
      "learning_rate": 0.00018216901911738434,
      "loss": 1.8181,
      "step": 600
    },
    {
      "epoch": 0.32126398946675444,
      "grad_norm": 0.5799180865287781,
      "learning_rate": 0.00018181168483115957,
      "loss": 1.8506,
      "step": 610
    },
    {
      "epoch": 0.32653061224489793,
      "grad_norm": 0.6307955980300903,
      "learning_rate": 0.0001814543505449348,
      "loss": 1.8169,
      "step": 620
    },
    {
      "epoch": 0.3317972350230415,
      "grad_norm": 0.5700279474258423,
      "learning_rate": 0.00018109701625871003,
      "loss": 1.781,
      "step": 630
    },
    {
      "epoch": 0.337063857801185,
      "grad_norm": 0.5440431237220764,
      "learning_rate": 0.00018073968197248528,
      "loss": 1.8269,
      "step": 640
    },
    {
      "epoch": 0.3423304805793285,
      "grad_norm": 0.5772846341133118,
      "learning_rate": 0.0001803823476862605,
      "loss": 1.7714,
      "step": 650
    },
    {
      "epoch": 0.347597103357472,
      "grad_norm": 0.5181092023849487,
      "learning_rate": 0.00018002501340003575,
      "loss": 1.8014,
      "step": 660
    },
    {
      "epoch": 0.35286372613561556,
      "grad_norm": 0.634819507598877,
      "learning_rate": 0.00017966767911381098,
      "loss": 1.7995,
      "step": 670
    },
    {
      "epoch": 0.35813034891375906,
      "grad_norm": 0.5963437557220459,
      "learning_rate": 0.0001793103448275862,
      "loss": 1.8045,
      "step": 680
    },
    {
      "epoch": 0.36339697169190255,
      "grad_norm": 0.567425012588501,
      "learning_rate": 0.00017895301054136144,
      "loss": 1.7864,
      "step": 690
    },
    {
      "epoch": 0.3686635944700461,
      "grad_norm": 0.6763649582862854,
      "learning_rate": 0.0001785956762551367,
      "loss": 1.8244,
      "step": 700
    },
    {
      "epoch": 0.3739302172481896,
      "grad_norm": 0.6015793681144714,
      "learning_rate": 0.00017823834196891192,
      "loss": 1.8782,
      "step": 710
    },
    {
      "epoch": 0.37919684002633314,
      "grad_norm": 0.5668109655380249,
      "learning_rate": 0.00017788100768268716,
      "loss": 1.7718,
      "step": 720
    },
    {
      "epoch": 0.38446346280447663,
      "grad_norm": 0.5689749121665955,
      "learning_rate": 0.0001775236733964624,
      "loss": 1.7671,
      "step": 730
    },
    {
      "epoch": 0.3897300855826201,
      "grad_norm": 0.5897608995437622,
      "learning_rate": 0.00017716633911023764,
      "loss": 1.859,
      "step": 740
    },
    {
      "epoch": 0.39499670836076367,
      "grad_norm": 0.5770469903945923,
      "learning_rate": 0.00017680900482401287,
      "loss": 1.8159,
      "step": 750
    },
    {
      "epoch": 0.40026333113890716,
      "grad_norm": 0.5897888541221619,
      "learning_rate": 0.0001764516705377881,
      "loss": 1.7601,
      "step": 760
    },
    {
      "epoch": 0.4055299539170507,
      "grad_norm": 0.5682525038719177,
      "learning_rate": 0.00017609433625156336,
      "loss": 1.8386,
      "step": 770
    },
    {
      "epoch": 0.4107965766951942,
      "grad_norm": 0.5978069305419922,
      "learning_rate": 0.00017573700196533857,
      "loss": 1.8068,
      "step": 780
    },
    {
      "epoch": 0.4160631994733377,
      "grad_norm": 0.5321393609046936,
      "learning_rate": 0.00017537966767911382,
      "loss": 1.8067,
      "step": 790
    },
    {
      "epoch": 0.42132982225148125,
      "grad_norm": 0.6343777179718018,
      "learning_rate": 0.00017502233339288905,
      "loss": 1.8207,
      "step": 800
    },
    {
      "epoch": 0.42659644502962474,
      "grad_norm": 0.5824087858200073,
      "learning_rate": 0.00017466499910666428,
      "loss": 1.7745,
      "step": 810
    },
    {
      "epoch": 0.4318630678077683,
      "grad_norm": 0.604559063911438,
      "learning_rate": 0.00017430766482043951,
      "loss": 1.8279,
      "step": 820
    },
    {
      "epoch": 0.4371296905859118,
      "grad_norm": 0.5716177225112915,
      "learning_rate": 0.00017395033053421477,
      "loss": 1.843,
      "step": 830
    },
    {
      "epoch": 0.4423963133640553,
      "grad_norm": 0.6541972160339355,
      "learning_rate": 0.00017359299624799,
      "loss": 1.826,
      "step": 840
    },
    {
      "epoch": 0.4476629361421988,
      "grad_norm": 0.5574677586555481,
      "learning_rate": 0.00017323566196176523,
      "loss": 1.7376,
      "step": 850
    },
    {
      "epoch": 0.4529295589203423,
      "grad_norm": 0.5444636940956116,
      "learning_rate": 0.0001728783276755405,
      "loss": 1.82,
      "step": 860
    },
    {
      "epoch": 0.45819618169848586,
      "grad_norm": 0.636162519454956,
      "learning_rate": 0.00017252099338931572,
      "loss": 1.839,
      "step": 870
    },
    {
      "epoch": 0.46346280447662935,
      "grad_norm": 0.6839801073074341,
      "learning_rate": 0.00017216365910309095,
      "loss": 1.8407,
      "step": 880
    },
    {
      "epoch": 0.4687294272547729,
      "grad_norm": 0.6598153710365295,
      "learning_rate": 0.00017180632481686618,
      "loss": 1.8033,
      "step": 890
    },
    {
      "epoch": 0.4739960500329164,
      "grad_norm": 0.5655220150947571,
      "learning_rate": 0.00017144899053064144,
      "loss": 1.6987,
      "step": 900
    },
    {
      "epoch": 0.4792626728110599,
      "grad_norm": 0.5713427662849426,
      "learning_rate": 0.00017109165624441664,
      "loss": 1.7871,
      "step": 910
    },
    {
      "epoch": 0.48452929558920343,
      "grad_norm": 0.6191885471343994,
      "learning_rate": 0.0001707343219581919,
      "loss": 1.834,
      "step": 920
    },
    {
      "epoch": 0.4897959183673469,
      "grad_norm": 0.6094474792480469,
      "learning_rate": 0.00017037698767196713,
      "loss": 1.7699,
      "step": 930
    },
    {
      "epoch": 0.4950625411454905,
      "grad_norm": 0.5948583483695984,
      "learning_rate": 0.00017001965338574236,
      "loss": 1.7696,
      "step": 940
    },
    {
      "epoch": 0.500329163923634,
      "grad_norm": 0.563062310218811,
      "learning_rate": 0.0001696623190995176,
      "loss": 1.8021,
      "step": 950
    },
    {
      "epoch": 0.5055957867017775,
      "grad_norm": 0.5827437043190002,
      "learning_rate": 0.00016930498481329285,
      "loss": 1.7578,
      "step": 960
    },
    {
      "epoch": 0.510862409479921,
      "grad_norm": 0.5648656487464905,
      "learning_rate": 0.00016894765052706808,
      "loss": 1.869,
      "step": 970
    },
    {
      "epoch": 0.5161290322580645,
      "grad_norm": 0.5812898874282837,
      "learning_rate": 0.0001685903162408433,
      "loss": 1.8111,
      "step": 980
    },
    {
      "epoch": 0.521395655036208,
      "grad_norm": 0.6257874965667725,
      "learning_rate": 0.00016823298195461857,
      "loss": 1.7025,
      "step": 990
    },
    {
      "epoch": 0.5266622778143516,
      "grad_norm": 0.9134134650230408,
      "learning_rate": 0.0001678756476683938,
      "loss": 1.8075,
      "step": 1000
    },
    {
      "epoch": 0.5266622778143516,
      "eval_loss": 1.8010175228118896,
      "eval_runtime": 2814.5108,
      "eval_samples_per_second": 0.6,
      "eval_steps_per_second": 0.6,
      "step": 1000
    },
    {
      "epoch": 0.5319289005924951,
      "grad_norm": 0.6623166799545288,
      "learning_rate": 0.00016751831338216903,
      "loss": 1.7595,
      "step": 1010
    },
    {
      "epoch": 0.5371955233706386,
      "grad_norm": 0.5989288091659546,
      "learning_rate": 0.00016716097909594426,
      "loss": 1.7636,
      "step": 1020
    },
    {
      "epoch": 0.5424621461487821,
      "grad_norm": 0.639840304851532,
      "learning_rate": 0.00016680364480971952,
      "loss": 1.795,
      "step": 1030
    },
    {
      "epoch": 0.5477287689269256,
      "grad_norm": 0.6623113751411438,
      "learning_rate": 0.00016644631052349472,
      "loss": 1.8375,
      "step": 1040
    },
    {
      "epoch": 0.5529953917050692,
      "grad_norm": 0.6113885045051575,
      "learning_rate": 0.00016608897623726998,
      "loss": 1.8027,
      "step": 1050
    },
    {
      "epoch": 0.5582620144832127,
      "grad_norm": 0.592003583908081,
      "learning_rate": 0.0001657316419510452,
      "loss": 1.7928,
      "step": 1060
    },
    {
      "epoch": 0.5635286372613562,
      "grad_norm": 0.7205474376678467,
      "learning_rate": 0.00016537430766482044,
      "loss": 1.7776,
      "step": 1070
    },
    {
      "epoch": 0.5687952600394997,
      "grad_norm": 0.679409921169281,
      "learning_rate": 0.00016501697337859567,
      "loss": 1.7952,
      "step": 1080
    },
    {
      "epoch": 0.5740618828176431,
      "grad_norm": 0.5873399376869202,
      "learning_rate": 0.00016465963909237093,
      "loss": 1.8287,
      "step": 1090
    },
    {
      "epoch": 0.5793285055957867,
      "grad_norm": 0.6025789380073547,
      "learning_rate": 0.00016430230480614616,
      "loss": 1.8011,
      "step": 1100
    },
    {
      "epoch": 0.5845951283739302,
      "grad_norm": 0.6075870990753174,
      "learning_rate": 0.0001639449705199214,
      "loss": 1.7937,
      "step": 1110
    },
    {
      "epoch": 0.5898617511520737,
      "grad_norm": 0.5936422348022461,
      "learning_rate": 0.00016358763623369664,
      "loss": 1.7139,
      "step": 1120
    },
    {
      "epoch": 0.5951283739302172,
      "grad_norm": 0.599635124206543,
      "learning_rate": 0.00016323030194747188,
      "loss": 1.8213,
      "step": 1130
    },
    {
      "epoch": 0.6003949967083607,
      "grad_norm": 0.6480646729469299,
      "learning_rate": 0.0001628729676612471,
      "loss": 1.7999,
      "step": 1140
    },
    {
      "epoch": 0.6056616194865043,
      "grad_norm": 0.6450600624084473,
      "learning_rate": 0.00016251563337502234,
      "loss": 1.8048,
      "step": 1150
    },
    {
      "epoch": 0.6109282422646478,
      "grad_norm": 0.6437613368034363,
      "learning_rate": 0.0001621582990887976,
      "loss": 1.9353,
      "step": 1160
    },
    {
      "epoch": 0.6161948650427913,
      "grad_norm": 0.6270756721496582,
      "learning_rate": 0.0001618009648025728,
      "loss": 1.7464,
      "step": 1170
    },
    {
      "epoch": 0.6214614878209348,
      "grad_norm": 0.5788578391075134,
      "learning_rate": 0.00016144363051634805,
      "loss": 1.7928,
      "step": 1180
    },
    {
      "epoch": 0.6267281105990783,
      "grad_norm": 0.5981865525245667,
      "learning_rate": 0.00016108629623012329,
      "loss": 1.8349,
      "step": 1190
    },
    {
      "epoch": 0.6319947333772219,
      "grad_norm": 0.644110381603241,
      "learning_rate": 0.00016072896194389852,
      "loss": 1.73,
      "step": 1200
    },
    {
      "epoch": 0.6372613561553654,
      "grad_norm": 0.5586547255516052,
      "learning_rate": 0.00016037162765767375,
      "loss": 1.8385,
      "step": 1210
    },
    {
      "epoch": 0.6425279789335089,
      "grad_norm": 0.6405012607574463,
      "learning_rate": 0.000160014293371449,
      "loss": 1.8314,
      "step": 1220
    },
    {
      "epoch": 0.6477946017116524,
      "grad_norm": 0.6557273864746094,
      "learning_rate": 0.00015965695908522423,
      "loss": 1.855,
      "step": 1230
    },
    {
      "epoch": 0.6530612244897959,
      "grad_norm": 0.6067843437194824,
      "learning_rate": 0.00015929962479899946,
      "loss": 1.871,
      "step": 1240
    },
    {
      "epoch": 0.6583278472679395,
      "grad_norm": 0.656855046749115,
      "learning_rate": 0.00015894229051277472,
      "loss": 1.7469,
      "step": 1250
    },
    {
      "epoch": 0.663594470046083,
      "grad_norm": 0.6445435285568237,
      "learning_rate": 0.00015858495622654995,
      "loss": 1.7969,
      "step": 1260
    },
    {
      "epoch": 0.6688610928242265,
      "grad_norm": 0.6318745613098145,
      "learning_rate": 0.00015822762194032518,
      "loss": 1.832,
      "step": 1270
    },
    {
      "epoch": 0.67412771560237,
      "grad_norm": 0.5600191354751587,
      "learning_rate": 0.00015787028765410041,
      "loss": 1.8018,
      "step": 1280
    },
    {
      "epoch": 0.6793943383805134,
      "grad_norm": 0.6417917609214783,
      "learning_rate": 0.00015751295336787567,
      "loss": 1.8469,
      "step": 1290
    },
    {
      "epoch": 0.684660961158657,
      "grad_norm": 0.6127220988273621,
      "learning_rate": 0.00015715561908165087,
      "loss": 1.7243,
      "step": 1300
    },
    {
      "epoch": 0.6899275839368005,
      "grad_norm": 0.6822757720947266,
      "learning_rate": 0.00015679828479542613,
      "loss": 1.7876,
      "step": 1310
    },
    {
      "epoch": 0.695194206714944,
      "grad_norm": 0.6336293816566467,
      "learning_rate": 0.00015644095050920136,
      "loss": 1.8091,
      "step": 1320
    },
    {
      "epoch": 0.7004608294930875,
      "grad_norm": 0.6000093221664429,
      "learning_rate": 0.0001560836162229766,
      "loss": 1.8895,
      "step": 1330
    },
    {
      "epoch": 0.7057274522712311,
      "grad_norm": 0.6353633999824524,
      "learning_rate": 0.00015572628193675182,
      "loss": 1.836,
      "step": 1340
    },
    {
      "epoch": 0.7109940750493746,
      "grad_norm": 0.6826469302177429,
      "learning_rate": 0.00015536894765052708,
      "loss": 1.7953,
      "step": 1350
    },
    {
      "epoch": 0.7162606978275181,
      "grad_norm": 0.6499598026275635,
      "learning_rate": 0.0001550116133643023,
      "loss": 1.8539,
      "step": 1360
    },
    {
      "epoch": 0.7215273206056616,
      "grad_norm": 0.5906498432159424,
      "learning_rate": 0.00015465427907807754,
      "loss": 1.6999,
      "step": 1370
    },
    {
      "epoch": 0.7267939433838051,
      "grad_norm": 0.6100245714187622,
      "learning_rate": 0.0001542969447918528,
      "loss": 1.8303,
      "step": 1380
    },
    {
      "epoch": 0.7320605661619487,
      "grad_norm": 0.6304928064346313,
      "learning_rate": 0.00015393961050562803,
      "loss": 1.8049,
      "step": 1390
    },
    {
      "epoch": 0.7373271889400922,
      "grad_norm": 0.6358779072761536,
      "learning_rate": 0.00015358227621940326,
      "loss": 1.7533,
      "step": 1400
    },
    {
      "epoch": 0.7425938117182357,
      "grad_norm": 0.6198335886001587,
      "learning_rate": 0.0001532249419331785,
      "loss": 1.7305,
      "step": 1410
    },
    {
      "epoch": 0.7478604344963792,
      "grad_norm": 0.6977216005325317,
      "learning_rate": 0.00015286760764695375,
      "loss": 1.8247,
      "step": 1420
    },
    {
      "epoch": 0.7531270572745227,
      "grad_norm": 0.6165383458137512,
      "learning_rate": 0.00015251027336072895,
      "loss": 1.6968,
      "step": 1430
    },
    {
      "epoch": 0.7583936800526663,
      "grad_norm": 0.7263792753219604,
      "learning_rate": 0.0001521529390745042,
      "loss": 1.6505,
      "step": 1440
    },
    {
      "epoch": 0.7636603028308098,
      "grad_norm": 0.6239280104637146,
      "learning_rate": 0.00015179560478827944,
      "loss": 1.7496,
      "step": 1450
    },
    {
      "epoch": 0.7689269256089533,
      "grad_norm": 0.6224576234817505,
      "learning_rate": 0.00015143827050205467,
      "loss": 1.756,
      "step": 1460
    },
    {
      "epoch": 0.7741935483870968,
      "grad_norm": 0.6684463620185852,
      "learning_rate": 0.00015108093621582993,
      "loss": 1.7862,
      "step": 1470
    },
    {
      "epoch": 0.7794601711652402,
      "grad_norm": 0.5826689004898071,
      "learning_rate": 0.00015072360192960516,
      "loss": 1.8396,
      "step": 1480
    },
    {
      "epoch": 0.7847267939433838,
      "grad_norm": 0.7196664214134216,
      "learning_rate": 0.0001503662676433804,
      "loss": 1.7339,
      "step": 1490
    },
    {
      "epoch": 0.7899934167215273,
      "grad_norm": 0.7067844867706299,
      "learning_rate": 0.00015000893335715562,
      "loss": 1.8116,
      "step": 1500
    },
    {
      "epoch": 0.7899934167215273,
      "eval_loss": 1.7870897054672241,
      "eval_runtime": 1956.6109,
      "eval_samples_per_second": 0.863,
      "eval_steps_per_second": 0.863,
      "step": 1500
    },
    {
      "epoch": 0.7952600394996708,
      "grad_norm": 0.6216100454330444,
      "learning_rate": 0.00014965159907093088,
      "loss": 1.7198,
      "step": 1510
    },
    {
      "epoch": 0.8005266622778143,
      "grad_norm": 0.6330670714378357,
      "learning_rate": 0.0001492942647847061,
      "loss": 1.7826,
      "step": 1520
    },
    {
      "epoch": 0.8057932850559578,
      "grad_norm": 0.7040934562683105,
      "learning_rate": 0.00014893693049848134,
      "loss": 1.759,
      "step": 1530
    },
    {
      "epoch": 0.8110599078341014,
      "grad_norm": 0.7231846451759338,
      "learning_rate": 0.00014857959621225657,
      "loss": 1.7757,
      "step": 1540
    },
    {
      "epoch": 0.8163265306122449,
      "grad_norm": 0.615455150604248,
      "learning_rate": 0.00014822226192603183,
      "loss": 1.8328,
      "step": 1550
    },
    {
      "epoch": 0.8215931533903884,
      "grad_norm": 0.6665547490119934,
      "learning_rate": 0.00014786492763980703,
      "loss": 1.7837,
      "step": 1560
    },
    {
      "epoch": 0.8268597761685319,
      "grad_norm": 0.6223162412643433,
      "learning_rate": 0.0001475075933535823,
      "loss": 1.7934,
      "step": 1570
    },
    {
      "epoch": 0.8321263989466754,
      "grad_norm": 0.6473308205604553,
      "learning_rate": 0.00014715025906735752,
      "loss": 1.7355,
      "step": 1580
    },
    {
      "epoch": 0.837393021724819,
      "grad_norm": 0.6808661818504333,
      "learning_rate": 0.00014679292478113275,
      "loss": 1.7855,
      "step": 1590
    },
    {
      "epoch": 0.8426596445029625,
      "grad_norm": 0.6262279152870178,
      "learning_rate": 0.000146435590494908,
      "loss": 1.8599,
      "step": 1600
    },
    {
      "epoch": 0.847926267281106,
      "grad_norm": 0.6396430134773254,
      "learning_rate": 0.00014607825620868324,
      "loss": 1.7449,
      "step": 1610
    },
    {
      "epoch": 0.8531928900592495,
      "grad_norm": 0.6375250816345215,
      "learning_rate": 0.00014572092192245847,
      "loss": 1.7673,
      "step": 1620
    },
    {
      "epoch": 0.858459512837393,
      "grad_norm": 0.6065658926963806,
      "learning_rate": 0.0001453635876362337,
      "loss": 1.7492,
      "step": 1630
    },
    {
      "epoch": 0.8637261356155366,
      "grad_norm": 0.6129201054573059,
      "learning_rate": 0.00014500625335000895,
      "loss": 1.8135,
      "step": 1640
    },
    {
      "epoch": 0.8689927583936801,
      "grad_norm": 0.729318380355835,
      "learning_rate": 0.00014464891906378419,
      "loss": 1.7724,
      "step": 1650
    },
    {
      "epoch": 0.8742593811718236,
      "grad_norm": 0.6677849888801575,
      "learning_rate": 0.00014429158477755942,
      "loss": 1.7391,
      "step": 1660
    },
    {
      "epoch": 0.879526003949967,
      "grad_norm": 0.6439751982688904,
      "learning_rate": 0.00014393425049133465,
      "loss": 1.8036,
      "step": 1670
    },
    {
      "epoch": 0.8847926267281107,
      "grad_norm": 0.6993604898452759,
      "learning_rate": 0.0001435769162051099,
      "loss": 1.852,
      "step": 1680
    },
    {
      "epoch": 0.8900592495062541,
      "grad_norm": 0.6758125424385071,
      "learning_rate": 0.0001432195819188851,
      "loss": 1.7626,
      "step": 1690
    },
    {
      "epoch": 0.8953258722843976,
      "grad_norm": 0.6063909530639648,
      "learning_rate": 0.00014286224763266036,
      "loss": 1.7655,
      "step": 1700
    },
    {
      "epoch": 0.9005924950625411,
      "grad_norm": 0.6268738508224487,
      "learning_rate": 0.0001425049133464356,
      "loss": 1.7267,
      "step": 1710
    },
    {
      "epoch": 0.9058591178406846,
      "grad_norm": 0.6511310935020447,
      "learning_rate": 0.00014214757906021083,
      "loss": 1.7343,
      "step": 1720
    },
    {
      "epoch": 0.9111257406188282,
      "grad_norm": 0.6856791377067566,
      "learning_rate": 0.00014179024477398608,
      "loss": 1.7437,
      "step": 1730
    },
    {
      "epoch": 0.9163923633969717,
      "grad_norm": 0.7004891633987427,
      "learning_rate": 0.0001414329104877613,
      "loss": 1.8143,
      "step": 1740
    },
    {
      "epoch": 0.9216589861751152,
      "grad_norm": 0.6972156763076782,
      "learning_rate": 0.00014107557620153654,
      "loss": 1.768,
      "step": 1750
    },
    {
      "epoch": 0.9269256089532587,
      "grad_norm": 0.5855867266654968,
      "learning_rate": 0.00014071824191531177,
      "loss": 1.8349,
      "step": 1760
    },
    {
      "epoch": 0.9321922317314022,
      "grad_norm": 0.6528839468955994,
      "learning_rate": 0.00014036090762908703,
      "loss": 1.7766,
      "step": 1770
    },
    {
      "epoch": 0.9374588545095458,
      "grad_norm": 0.6331847906112671,
      "learning_rate": 0.00014000357334286224,
      "loss": 1.7998,
      "step": 1780
    },
    {
      "epoch": 0.9427254772876893,
      "grad_norm": 0.6372424960136414,
      "learning_rate": 0.0001396462390566375,
      "loss": 1.7851,
      "step": 1790
    },
    {
      "epoch": 0.9479921000658328,
      "grad_norm": 0.6231042146682739,
      "learning_rate": 0.00013928890477041272,
      "loss": 1.8329,
      "step": 1800
    },
    {
      "epoch": 0.9532587228439763,
      "grad_norm": 0.6675165295600891,
      "learning_rate": 0.00013893157048418795,
      "loss": 1.7906,
      "step": 1810
    },
    {
      "epoch": 0.9585253456221198,
      "grad_norm": 0.7442963719367981,
      "learning_rate": 0.00013857423619796318,
      "loss": 1.9484,
      "step": 1820
    },
    {
      "epoch": 0.9637919684002634,
      "grad_norm": 0.6903140544891357,
      "learning_rate": 0.00013821690191173844,
      "loss": 1.7352,
      "step": 1830
    },
    {
      "epoch": 0.9690585911784069,
      "grad_norm": 0.6440390348434448,
      "learning_rate": 0.00013785956762551367,
      "loss": 1.8265,
      "step": 1840
    },
    {
      "epoch": 0.9743252139565504,
      "grad_norm": 0.7087055444717407,
      "learning_rate": 0.0001375022333392889,
      "loss": 1.7602,
      "step": 1850
    },
    {
      "epoch": 0.9795918367346939,
      "grad_norm": 0.7650249004364014,
      "learning_rate": 0.00013714489905306416,
      "loss": 1.7481,
      "step": 1860
    },
    {
      "epoch": 0.9848584595128373,
      "grad_norm": 0.6769753098487854,
      "learning_rate": 0.0001367875647668394,
      "loss": 1.7426,
      "step": 1870
    },
    {
      "epoch": 0.990125082290981,
      "grad_norm": 0.6345494985580444,
      "learning_rate": 0.00013643023048061462,
      "loss": 1.8278,
      "step": 1880
    },
    {
      "epoch": 0.9953917050691244,
      "grad_norm": 0.7039421200752258,
      "learning_rate": 0.00013607289619438985,
      "loss": 1.7892,
      "step": 1890
    },
    {
      "epoch": 1.0005266622778144,
      "grad_norm": 0.5930216312408447,
      "learning_rate": 0.0001357155619081651,
      "loss": 1.7624,
      "step": 1900
    },
    {
      "epoch": 1.0057932850559579,
      "grad_norm": 0.6079586744308472,
      "learning_rate": 0.0001353582276219403,
      "loss": 1.7567,
      "step": 1910
    },
    {
      "epoch": 1.0110599078341014,
      "grad_norm": 0.6603689193725586,
      "learning_rate": 0.00013500089333571557,
      "loss": 1.6631,
      "step": 1920
    },
    {
      "epoch": 1.0163265306122449,
      "grad_norm": 0.7148056626319885,
      "learning_rate": 0.0001346435590494908,
      "loss": 1.5893,
      "step": 1930
    },
    {
      "epoch": 1.0215931533903884,
      "grad_norm": 0.6488335728645325,
      "learning_rate": 0.00013428622476326603,
      "loss": 1.6476,
      "step": 1940
    },
    {
      "epoch": 1.0268597761685319,
      "grad_norm": 0.6831588745117188,
      "learning_rate": 0.00013392889047704126,
      "loss": 1.7938,
      "step": 1950
    },
    {
      "epoch": 1.0321263989466753,
      "grad_norm": 0.6735628843307495,
      "learning_rate": 0.00013357155619081652,
      "loss": 1.6634,
      "step": 1960
    },
    {
      "epoch": 1.037393021724819,
      "grad_norm": 0.680339515209198,
      "learning_rate": 0.00013321422190459175,
      "loss": 1.5821,
      "step": 1970
    },
    {
      "epoch": 1.0426596445029626,
      "grad_norm": 0.9483171105384827,
      "learning_rate": 0.00013285688761836698,
      "loss": 1.6491,
      "step": 1980
    },
    {
      "epoch": 1.047926267281106,
      "grad_norm": 0.6694039106369019,
      "learning_rate": 0.00013249955333214224,
      "loss": 1.7406,
      "step": 1990
    },
    {
      "epoch": 1.0531928900592495,
      "grad_norm": 0.6402232050895691,
      "learning_rate": 0.00013214221904591747,
      "loss": 1.8039,
      "step": 2000
    },
    {
      "epoch": 1.0531928900592495,
      "eval_loss": 1.7795095443725586,
      "eval_runtime": 1956.6479,
      "eval_samples_per_second": 0.863,
      "eval_steps_per_second": 0.863,
      "step": 2000
    },
    {
      "epoch": 1.058459512837393,
      "grad_norm": 0.7238403558731079,
      "learning_rate": 0.0001317848847596927,
      "loss": 1.6713,
      "step": 2010
    },
    {
      "epoch": 1.0637261356155365,
      "grad_norm": 0.7051423788070679,
      "learning_rate": 0.00013142755047346793,
      "loss": 1.6694,
      "step": 2020
    },
    {
      "epoch": 1.06899275839368,
      "grad_norm": 0.7147190570831299,
      "learning_rate": 0.0001310702161872432,
      "loss": 1.7064,
      "step": 2030
    },
    {
      "epoch": 1.0742593811718235,
      "grad_norm": 0.7152087688446045,
      "learning_rate": 0.0001307128819010184,
      "loss": 1.6839,
      "step": 2040
    },
    {
      "epoch": 1.079526003949967,
      "grad_norm": 0.6766934394836426,
      "learning_rate": 0.00013035554761479365,
      "loss": 1.7201,
      "step": 2050
    },
    {
      "epoch": 1.0847926267281105,
      "grad_norm": 0.6764931678771973,
      "learning_rate": 0.00012999821332856888,
      "loss": 1.6364,
      "step": 2060
    },
    {
      "epoch": 1.0900592495062542,
      "grad_norm": 0.7250349521636963,
      "learning_rate": 0.0001296408790423441,
      "loss": 1.6605,
      "step": 2070
    },
    {
      "epoch": 1.0953258722843977,
      "grad_norm": 0.7048836946487427,
      "learning_rate": 0.00012928354475611934,
      "loss": 1.7383,
      "step": 2080
    },
    {
      "epoch": 1.1005924950625412,
      "grad_norm": 0.6902994513511658,
      "learning_rate": 0.0001289262104698946,
      "loss": 1.6607,
      "step": 2090
    },
    {
      "epoch": 1.1058591178406847,
      "grad_norm": 0.6957078576087952,
      "learning_rate": 0.00012856887618366983,
      "loss": 1.6745,
      "step": 2100
    },
    {
      "epoch": 1.1111257406188282,
      "grad_norm": 0.7217925786972046,
      "learning_rate": 0.00012821154189744506,
      "loss": 1.6628,
      "step": 2110
    },
    {
      "epoch": 1.1163923633969717,
      "grad_norm": 0.7246092557907104,
      "learning_rate": 0.00012785420761122032,
      "loss": 1.6481,
      "step": 2120
    },
    {
      "epoch": 1.1216589861751152,
      "grad_norm": 0.7650777697563171,
      "learning_rate": 0.00012749687332499555,
      "loss": 1.6578,
      "step": 2130
    },
    {
      "epoch": 1.1269256089532587,
      "grad_norm": 0.6614729166030884,
      "learning_rate": 0.00012713953903877078,
      "loss": 1.7949,
      "step": 2140
    },
    {
      "epoch": 1.1321922317314022,
      "grad_norm": 0.742185115814209,
      "learning_rate": 0.000126782204752546,
      "loss": 1.6565,
      "step": 2150
    },
    {
      "epoch": 1.1374588545095459,
      "grad_norm": 0.7298918962478638,
      "learning_rate": 0.00012642487046632126,
      "loss": 1.7398,
      "step": 2160
    },
    {
      "epoch": 1.1427254772876894,
      "grad_norm": 0.7760581970214844,
      "learning_rate": 0.00012606753618009647,
      "loss": 1.6411,
      "step": 2170
    },
    {
      "epoch": 1.1479921000658329,
      "grad_norm": 0.7241498827934265,
      "learning_rate": 0.00012571020189387173,
      "loss": 1.7148,
      "step": 2180
    },
    {
      "epoch": 1.1532587228439763,
      "grad_norm": 0.7165136933326721,
      "learning_rate": 0.00012535286760764696,
      "loss": 1.6607,
      "step": 2190
    },
    {
      "epoch": 1.1585253456221198,
      "grad_norm": 0.7206676602363586,
      "learning_rate": 0.00012499553332142219,
      "loss": 1.6793,
      "step": 2200
    },
    {
      "epoch": 1.1637919684002633,
      "grad_norm": 0.7209619283676147,
      "learning_rate": 0.00012463819903519742,
      "loss": 1.7142,
      "step": 2210
    },
    {
      "epoch": 1.1690585911784068,
      "grad_norm": 0.7396821975708008,
      "learning_rate": 0.00012428086474897267,
      "loss": 1.661,
      "step": 2220
    },
    {
      "epoch": 1.1743252139565503,
      "grad_norm": 0.6442410349845886,
      "learning_rate": 0.0001239235304627479,
      "loss": 1.6493,
      "step": 2230
    },
    {
      "epoch": 1.1795918367346938,
      "grad_norm": 0.7238087058067322,
      "learning_rate": 0.00012356619617652314,
      "loss": 1.7323,
      "step": 2240
    },
    {
      "epoch": 1.1848584595128373,
      "grad_norm": 0.7211587429046631,
      "learning_rate": 0.0001232088618902984,
      "loss": 1.7197,
      "step": 2250
    },
    {
      "epoch": 1.1901250822909808,
      "grad_norm": 0.7476163506507874,
      "learning_rate": 0.00012285152760407362,
      "loss": 1.6712,
      "step": 2260
    },
    {
      "epoch": 1.1953917050691245,
      "grad_norm": 0.7131553888320923,
      "learning_rate": 0.00012249419331784885,
      "loss": 1.6899,
      "step": 2270
    },
    {
      "epoch": 1.200658327847268,
      "grad_norm": 0.6851919293403625,
      "learning_rate": 0.00012213685903162408,
      "loss": 1.6525,
      "step": 2280
    },
    {
      "epoch": 1.2059249506254115,
      "grad_norm": 0.7383894324302673,
      "learning_rate": 0.00012177952474539933,
      "loss": 1.5966,
      "step": 2290
    },
    {
      "epoch": 1.211191573403555,
      "grad_norm": 0.6809538006782532,
      "learning_rate": 0.00012142219045917456,
      "loss": 1.7238,
      "step": 2300
    },
    {
      "epoch": 1.2164581961816985,
      "grad_norm": 0.6693501472473145,
      "learning_rate": 0.0001210648561729498,
      "loss": 1.7339,
      "step": 2310
    },
    {
      "epoch": 1.221724818959842,
      "grad_norm": 0.6906978487968445,
      "learning_rate": 0.00012070752188672503,
      "loss": 1.6858,
      "step": 2320
    },
    {
      "epoch": 1.2269914417379855,
      "grad_norm": 0.7001760005950928,
      "learning_rate": 0.00012035018760050028,
      "loss": 1.696,
      "step": 2330
    },
    {
      "epoch": 1.232258064516129,
      "grad_norm": 0.6825568675994873,
      "learning_rate": 0.0001199928533142755,
      "loss": 1.641,
      "step": 2340
    },
    {
      "epoch": 1.2375246872942725,
      "grad_norm": 0.6819080710411072,
      "learning_rate": 0.00011963551902805075,
      "loss": 1.7624,
      "step": 2350
    },
    {
      "epoch": 1.2427913100724162,
      "grad_norm": 0.7472482323646545,
      "learning_rate": 0.000119278184741826,
      "loss": 1.707,
      "step": 2360
    },
    {
      "epoch": 1.2480579328505597,
      "grad_norm": 0.7514809966087341,
      "learning_rate": 0.00011892085045560121,
      "loss": 1.6996,
      "step": 2370
    },
    {
      "epoch": 1.2533245556287032,
      "grad_norm": 0.7022672891616821,
      "learning_rate": 0.00011856351616937647,
      "loss": 1.7206,
      "step": 2380
    },
    {
      "epoch": 1.2585911784068466,
      "grad_norm": 0.7528488039970398,
      "learning_rate": 0.00011820618188315169,
      "loss": 1.6842,
      "step": 2390
    },
    {
      "epoch": 1.2638578011849901,
      "grad_norm": 0.7878667116165161,
      "learning_rate": 0.00011784884759692693,
      "loss": 1.6455,
      "step": 2400
    },
    {
      "epoch": 1.2691244239631336,
      "grad_norm": 0.7072049379348755,
      "learning_rate": 0.00011749151331070216,
      "loss": 1.704,
      "step": 2410
    },
    {
      "epoch": 1.2743910467412771,
      "grad_norm": 0.7041506171226501,
      "learning_rate": 0.0001171341790244774,
      "loss": 1.6105,
      "step": 2420
    },
    {
      "epoch": 1.2796576695194206,
      "grad_norm": 0.7800479531288147,
      "learning_rate": 0.00011677684473825264,
      "loss": 1.6499,
      "step": 2430
    },
    {
      "epoch": 1.284924292297564,
      "grad_norm": 0.6645513772964478,
      "learning_rate": 0.00011641951045202788,
      "loss": 1.831,
      "step": 2440
    },
    {
      "epoch": 1.2901909150757076,
      "grad_norm": 0.7494238018989563,
      "learning_rate": 0.00011606217616580311,
      "loss": 1.6878,
      "step": 2450
    },
    {
      "epoch": 1.295457537853851,
      "grad_norm": 0.7395976781845093,
      "learning_rate": 0.00011570484187957835,
      "loss": 1.6524,
      "step": 2460
    },
    {
      "epoch": 1.3007241606319948,
      "grad_norm": 0.8216167688369751,
      "learning_rate": 0.0001153475075933536,
      "loss": 1.7529,
      "step": 2470
    },
    {
      "epoch": 1.3059907834101383,
      "grad_norm": 0.7854958772659302,
      "learning_rate": 0.00011499017330712883,
      "loss": 1.6827,
      "step": 2480
    },
    {
      "epoch": 1.3112574061882818,
      "grad_norm": 0.7090415358543396,
      "learning_rate": 0.00011463283902090407,
      "loss": 1.6452,
      "step": 2490
    },
    {
      "epoch": 1.3165240289664253,
      "grad_norm": 0.7391517758369446,
      "learning_rate": 0.00011427550473467929,
      "loss": 1.6798,
      "step": 2500
    },
    {
      "epoch": 1.3165240289664253,
      "eval_loss": 1.775834321975708,
      "eval_runtime": 2595.9426,
      "eval_samples_per_second": 0.65,
      "eval_steps_per_second": 0.65,
      "step": 2500
    },
    {
      "epoch": 1.3217906517445688,
      "grad_norm": 0.7659851312637329,
      "learning_rate": 0.00011391817044845455,
      "loss": 1.6969,
      "step": 2510
    },
    {
      "epoch": 1.3270572745227123,
      "grad_norm": 0.7475526928901672,
      "learning_rate": 0.00011356083616222976,
      "loss": 1.6595,
      "step": 2520
    },
    {
      "epoch": 1.3323238973008558,
      "grad_norm": 0.756208062171936,
      "learning_rate": 0.00011320350187600501,
      "loss": 1.701,
      "step": 2530
    },
    {
      "epoch": 1.3375905200789995,
      "grad_norm": 0.7283508777618408,
      "learning_rate": 0.00011284616758978024,
      "loss": 1.6768,
      "step": 2540
    },
    {
      "epoch": 1.342857142857143,
      "grad_norm": 0.7164903283119202,
      "learning_rate": 0.00011248883330355548,
      "loss": 1.6409,
      "step": 2550
    },
    {
      "epoch": 1.3481237656352865,
      "grad_norm": 0.6782122850418091,
      "learning_rate": 0.00011213149901733071,
      "loss": 1.6808,
      "step": 2560
    },
    {
      "epoch": 1.35339038841343,
      "grad_norm": 0.7561405897140503,
      "learning_rate": 0.00011177416473110596,
      "loss": 1.6286,
      "step": 2570
    },
    {
      "epoch": 1.3586570111915734,
      "grad_norm": 0.7155987024307251,
      "learning_rate": 0.00011141683044488119,
      "loss": 1.727,
      "step": 2580
    },
    {
      "epoch": 1.363923633969717,
      "grad_norm": 0.7537088394165039,
      "learning_rate": 0.00011105949615865643,
      "loss": 1.6386,
      "step": 2590
    },
    {
      "epoch": 1.3691902567478604,
      "grad_norm": 0.7428790330886841,
      "learning_rate": 0.00011070216187243168,
      "loss": 1.6708,
      "step": 2600
    },
    {
      "epoch": 1.374456879526004,
      "grad_norm": 0.7820054292678833,
      "learning_rate": 0.0001103448275862069,
      "loss": 1.7417,
      "step": 2610
    },
    {
      "epoch": 1.3797235023041474,
      "grad_norm": 0.7303804755210876,
      "learning_rate": 0.00010998749329998215,
      "loss": 1.6972,
      "step": 2620
    },
    {
      "epoch": 1.384990125082291,
      "grad_norm": 0.8039243221282959,
      "learning_rate": 0.00010963015901375737,
      "loss": 1.6944,
      "step": 2630
    },
    {
      "epoch": 1.3902567478604344,
      "grad_norm": 0.7143170237541199,
      "learning_rate": 0.00010927282472753263,
      "loss": 1.6741,
      "step": 2640
    },
    {
      "epoch": 1.395523370638578,
      "grad_norm": 0.7816708087921143,
      "learning_rate": 0.00010891549044130784,
      "loss": 1.6186,
      "step": 2650
    },
    {
      "epoch": 1.4007899934167214,
      "grad_norm": 0.7109012603759766,
      "learning_rate": 0.00010855815615508309,
      "loss": 1.6915,
      "step": 2660
    },
    {
      "epoch": 1.406056616194865,
      "grad_norm": 0.7644191384315491,
      "learning_rate": 0.00010820082186885832,
      "loss": 1.7262,
      "step": 2670
    },
    {
      "epoch": 1.4113232389730086,
      "grad_norm": 0.7857317328453064,
      "learning_rate": 0.00010784348758263356,
      "loss": 1.7394,
      "step": 2680
    },
    {
      "epoch": 1.416589861751152,
      "grad_norm": 0.7314788699150085,
      "learning_rate": 0.00010748615329640879,
      "loss": 1.6397,
      "step": 2690
    },
    {
      "epoch": 1.4218564845292956,
      "grad_norm": 0.7520400881767273,
      "learning_rate": 0.00010712881901018403,
      "loss": 1.7106,
      "step": 2700
    },
    {
      "epoch": 1.427123107307439,
      "grad_norm": 0.7610954642295837,
      "learning_rate": 0.00010677148472395925,
      "loss": 1.7576,
      "step": 2710
    },
    {
      "epoch": 1.4323897300855826,
      "grad_norm": 0.684325635433197,
      "learning_rate": 0.00010641415043773451,
      "loss": 1.7315,
      "step": 2720
    },
    {
      "epoch": 1.437656352863726,
      "grad_norm": 0.8673454523086548,
      "learning_rate": 0.00010605681615150975,
      "loss": 1.6679,
      "step": 2730
    },
    {
      "epoch": 1.4429229756418698,
      "grad_norm": 0.8145061135292053,
      "learning_rate": 0.00010569948186528498,
      "loss": 1.6661,
      "step": 2740
    },
    {
      "epoch": 1.4481895984200133,
      "grad_norm": 0.7424800992012024,
      "learning_rate": 0.00010534214757906023,
      "loss": 1.7393,
      "step": 2750
    },
    {
      "epoch": 1.4534562211981568,
      "grad_norm": 0.6944065690040588,
      "learning_rate": 0.00010498481329283544,
      "loss": 1.6494,
      "step": 2760
    },
    {
      "epoch": 1.4587228439763003,
      "grad_norm": 0.7329607009887695,
      "learning_rate": 0.0001046274790066107,
      "loss": 1.6988,
      "step": 2770
    },
    {
      "epoch": 1.4639894667544437,
      "grad_norm": 0.7531055212020874,
      "learning_rate": 0.00010427014472038592,
      "loss": 1.7415,
      "step": 2780
    },
    {
      "epoch": 1.4692560895325872,
      "grad_norm": 0.7194845080375671,
      "learning_rate": 0.00010391281043416116,
      "loss": 1.7458,
      "step": 2790
    },
    {
      "epoch": 1.4745227123107307,
      "grad_norm": 0.7774937152862549,
      "learning_rate": 0.0001035554761479364,
      "loss": 1.6791,
      "step": 2800
    },
    {
      "epoch": 1.4797893350888742,
      "grad_norm": 0.835082471370697,
      "learning_rate": 0.00010319814186171164,
      "loss": 1.7221,
      "step": 2810
    },
    {
      "epoch": 1.4850559578670177,
      "grad_norm": 0.7729910016059875,
      "learning_rate": 0.00010284080757548687,
      "loss": 1.615,
      "step": 2820
    },
    {
      "epoch": 1.4903225806451612,
      "grad_norm": 0.7066395282745361,
      "learning_rate": 0.00010248347328926211,
      "loss": 1.6776,
      "step": 2830
    },
    {
      "epoch": 1.4955892034233047,
      "grad_norm": 0.8216099739074707,
      "learning_rate": 0.00010212613900303733,
      "loss": 1.7327,
      "step": 2840
    },
    {
      "epoch": 1.5008558262014482,
      "grad_norm": 0.7964605689048767,
      "learning_rate": 0.00010176880471681259,
      "loss": 1.7193,
      "step": 2850
    },
    {
      "epoch": 1.5061224489795917,
      "grad_norm": 0.7979776263237,
      "learning_rate": 0.00010141147043058783,
      "loss": 1.6417,
      "step": 2860
    },
    {
      "epoch": 1.5113890717577354,
      "grad_norm": 0.7211545705795288,
      "learning_rate": 0.00010105413614436305,
      "loss": 1.6594,
      "step": 2870
    },
    {
      "epoch": 1.516655694535879,
      "grad_norm": 0.7673373818397522,
      "learning_rate": 0.0001006968018581383,
      "loss": 1.7356,
      "step": 2880
    },
    {
      "epoch": 1.5219223173140224,
      "grad_norm": 0.7788179516792297,
      "learning_rate": 0.00010033946757191352,
      "loss": 1.6522,
      "step": 2890
    },
    {
      "epoch": 1.5271889400921659,
      "grad_norm": 0.7781999707221985,
      "learning_rate": 9.998213328568877e-05,
      "loss": 1.7237,
      "step": 2900
    },
    {
      "epoch": 1.5324555628703094,
      "grad_norm": 0.7606337666511536,
      "learning_rate": 9.962479899946401e-05,
      "loss": 1.7377,
      "step": 2910
    },
    {
      "epoch": 1.537722185648453,
      "grad_norm": 0.7484115958213806,
      "learning_rate": 9.926746471323924e-05,
      "loss": 1.7164,
      "step": 2920
    },
    {
      "epoch": 1.5429888084265966,
      "grad_norm": 0.7998282313346863,
      "learning_rate": 9.891013042701448e-05,
      "loss": 1.6609,
      "step": 2930
    },
    {
      "epoch": 1.54825543120474,
      "grad_norm": 0.7249443531036377,
      "learning_rate": 9.855279614078972e-05,
      "loss": 1.6469,
      "step": 2940
    },
    {
      "epoch": 1.5535220539828836,
      "grad_norm": 0.7720962762832642,
      "learning_rate": 9.819546185456495e-05,
      "loss": 1.7104,
      "step": 2950
    },
    {
      "epoch": 1.558788676761027,
      "grad_norm": 0.7630874514579773,
      "learning_rate": 9.783812756834019e-05,
      "loss": 1.7374,
      "step": 2960
    },
    {
      "epoch": 1.5640552995391706,
      "grad_norm": 0.7611251473426819,
      "learning_rate": 9.748079328211542e-05,
      "loss": 1.7065,
      "step": 2970
    },
    {
      "epoch": 1.569321922317314,
      "grad_norm": 0.7624449729919434,
      "learning_rate": 9.712345899589066e-05,
      "loss": 1.6955,
      "step": 2980
    },
    {
      "epoch": 1.5745885450954575,
      "grad_norm": 0.7250643372535706,
      "learning_rate": 9.67661247096659e-05,
      "loss": 1.7346,
      "step": 2990
    },
    {
      "epoch": 1.579855167873601,
      "grad_norm": 0.7460132837295532,
      "learning_rate": 9.640879042344113e-05,
      "loss": 1.7276,
      "step": 3000
    },
    {
      "epoch": 1.579855167873601,
      "eval_loss": 1.766935110092163,
      "eval_runtime": 1935.3726,
      "eval_samples_per_second": 0.872,
      "eval_steps_per_second": 0.872,
      "step": 3000
    },
    {
      "epoch": 1.5851217906517445,
      "grad_norm": 0.7591244578361511,
      "learning_rate": 9.605145613721637e-05,
      "loss": 1.645,
      "step": 3010
    },
    {
      "epoch": 1.590388413429888,
      "grad_norm": 0.7239627242088318,
      "learning_rate": 9.56941218509916e-05,
      "loss": 1.6702,
      "step": 3020
    },
    {
      "epoch": 1.5956550362080315,
      "grad_norm": 0.7406943440437317,
      "learning_rate": 9.533678756476684e-05,
      "loss": 1.6914,
      "step": 3030
    },
    {
      "epoch": 1.600921658986175,
      "grad_norm": 0.7661581039428711,
      "learning_rate": 9.497945327854209e-05,
      "loss": 1.7323,
      "step": 3040
    },
    {
      "epoch": 1.6061882817643185,
      "grad_norm": 0.9081594944000244,
      "learning_rate": 9.462211899231732e-05,
      "loss": 1.8067,
      "step": 3050
    },
    {
      "epoch": 1.611454904542462,
      "grad_norm": 0.7973509430885315,
      "learning_rate": 9.426478470609256e-05,
      "loss": 1.7454,
      "step": 3060
    },
    {
      "epoch": 1.6167215273206057,
      "grad_norm": 0.7683377265930176,
      "learning_rate": 9.390745041986779e-05,
      "loss": 1.6793,
      "step": 3070
    },
    {
      "epoch": 1.6219881500987492,
      "grad_norm": 0.7657972574234009,
      "learning_rate": 9.355011613364302e-05,
      "loss": 1.6736,
      "step": 3080
    },
    {
      "epoch": 1.6272547728768927,
      "grad_norm": 0.756361186504364,
      "learning_rate": 9.319278184741827e-05,
      "loss": 1.6274,
      "step": 3090
    },
    {
      "epoch": 1.6325213956550362,
      "grad_norm": 0.7661951184272766,
      "learning_rate": 9.28354475611935e-05,
      "loss": 1.7774,
      "step": 3100
    },
    {
      "epoch": 1.6377880184331797,
      "grad_norm": 0.766514778137207,
      "learning_rate": 9.247811327496874e-05,
      "loss": 1.7219,
      "step": 3110
    },
    {
      "epoch": 1.6430546412113234,
      "grad_norm": 0.7623353600502014,
      "learning_rate": 9.212077898874397e-05,
      "loss": 1.6742,
      "step": 3120
    },
    {
      "epoch": 1.6483212639894669,
      "grad_norm": 0.7124127745628357,
      "learning_rate": 9.17634447025192e-05,
      "loss": 1.6154,
      "step": 3130
    },
    {
      "epoch": 1.6535878867676104,
      "grad_norm": 0.762725293636322,
      "learning_rate": 9.140611041629445e-05,
      "loss": 1.6617,
      "step": 3140
    },
    {
      "epoch": 1.6588545095457539,
      "grad_norm": 0.7679426670074463,
      "learning_rate": 9.104877613006968e-05,
      "loss": 1.6949,
      "step": 3150
    },
    {
      "epoch": 1.6641211323238974,
      "grad_norm": 0.83601313829422,
      "learning_rate": 9.069144184384492e-05,
      "loss": 1.6103,
      "step": 3160
    },
    {
      "epoch": 1.6693877551020408,
      "grad_norm": 0.7911811470985413,
      "learning_rate": 9.033410755762017e-05,
      "loss": 1.6761,
      "step": 3170
    },
    {
      "epoch": 1.6746543778801843,
      "grad_norm": 0.7412975430488586,
      "learning_rate": 8.99767732713954e-05,
      "loss": 1.6364,
      "step": 3180
    },
    {
      "epoch": 1.6799210006583278,
      "grad_norm": 1.0399025678634644,
      "learning_rate": 8.961943898517064e-05,
      "loss": 1.6797,
      "step": 3190
    },
    {
      "epoch": 1.6851876234364713,
      "grad_norm": 0.6909993886947632,
      "learning_rate": 8.926210469894587e-05,
      "loss": 1.7128,
      "step": 3200
    },
    {
      "epoch": 1.6904542462146148,
      "grad_norm": 0.7982816696166992,
      "learning_rate": 8.89047704127211e-05,
      "loss": 1.6651,
      "step": 3210
    },
    {
      "epoch": 1.6957208689927583,
      "grad_norm": 0.7204341888427734,
      "learning_rate": 8.854743612649634e-05,
      "loss": 1.7321,
      "step": 3220
    },
    {
      "epoch": 1.7009874917709018,
      "grad_norm": 0.8177319765090942,
      "learning_rate": 8.819010184027158e-05,
      "loss": 1.6917,
      "step": 3230
    },
    {
      "epoch": 1.7062541145490453,
      "grad_norm": 0.73216313123703,
      "learning_rate": 8.783276755404682e-05,
      "loss": 1.6135,
      "step": 3240
    },
    {
      "epoch": 1.7115207373271888,
      "grad_norm": 0.7718359231948853,
      "learning_rate": 8.747543326782205e-05,
      "loss": 1.5733,
      "step": 3250
    },
    {
      "epoch": 1.7167873601053325,
      "grad_norm": 0.6911469101905823,
      "learning_rate": 8.711809898159728e-05,
      "loss": 1.6683,
      "step": 3260
    },
    {
      "epoch": 1.722053982883476,
      "grad_norm": 0.7687360048294067,
      "learning_rate": 8.676076469537252e-05,
      "loss": 1.7093,
      "step": 3270
    },
    {
      "epoch": 1.7273206056616195,
      "grad_norm": 0.746009886264801,
      "learning_rate": 8.640343040914775e-05,
      "loss": 1.6709,
      "step": 3280
    },
    {
      "epoch": 1.732587228439763,
      "grad_norm": 0.8120574951171875,
      "learning_rate": 8.6046096122923e-05,
      "loss": 1.6854,
      "step": 3290
    },
    {
      "epoch": 1.7378538512179065,
      "grad_norm": 0.8052160739898682,
      "learning_rate": 8.568876183669824e-05,
      "loss": 1.706,
      "step": 3300
    },
    {
      "epoch": 1.7431204739960502,
      "grad_norm": 0.6563901305198669,
      "learning_rate": 8.533142755047347e-05,
      "loss": 1.7,
      "step": 3310
    },
    {
      "epoch": 1.7483870967741937,
      "grad_norm": 0.7574971318244934,
      "learning_rate": 8.497409326424872e-05,
      "loss": 1.6383,
      "step": 3320
    },
    {
      "epoch": 1.7536537195523372,
      "grad_norm": 0.793493926525116,
      "learning_rate": 8.461675897802395e-05,
      "loss": 1.6512,
      "step": 3330
    },
    {
      "epoch": 1.7589203423304807,
      "grad_norm": 0.7956052422523499,
      "learning_rate": 8.425942469179918e-05,
      "loss": 1.7499,
      "step": 3340
    },
    {
      "epoch": 1.7641869651086242,
      "grad_norm": 0.7816707491874695,
      "learning_rate": 8.390209040557442e-05,
      "loss": 1.7735,
      "step": 3350
    },
    {
      "epoch": 1.7694535878867677,
      "grad_norm": 0.7691996097564697,
      "learning_rate": 8.354475611934965e-05,
      "loss": 1.6869,
      "step": 3360
    },
    {
      "epoch": 1.7747202106649111,
      "grad_norm": 0.8393162488937378,
      "learning_rate": 8.31874218331249e-05,
      "loss": 1.6999,
      "step": 3370
    },
    {
      "epoch": 1.7799868334430546,
      "grad_norm": 0.7500560879707336,
      "learning_rate": 8.283008754690013e-05,
      "loss": 1.7405,
      "step": 3380
    },
    {
      "epoch": 1.7852534562211981,
      "grad_norm": 0.7411943674087524,
      "learning_rate": 8.247275326067536e-05,
      "loss": 1.7188,
      "step": 3390
    },
    {
      "epoch": 1.7905200789993416,
      "grad_norm": 0.7446027398109436,
      "learning_rate": 8.21154189744506e-05,
      "loss": 1.6589,
      "step": 3400
    },
    {
      "epoch": 1.7957867017774851,
      "grad_norm": 0.7500229477882385,
      "learning_rate": 8.175808468822585e-05,
      "loss": 1.7628,
      "step": 3410
    },
    {
      "epoch": 1.8010533245556286,
      "grad_norm": 0.7353859543800354,
      "learning_rate": 8.140075040200108e-05,
      "loss": 1.7017,
      "step": 3420
    },
    {
      "epoch": 1.806319947333772,
      "grad_norm": 0.6963145136833191,
      "learning_rate": 8.104341611577632e-05,
      "loss": 1.6677,
      "step": 3430
    },
    {
      "epoch": 1.8115865701119156,
      "grad_norm": 0.7354899644851685,
      "learning_rate": 8.068608182955155e-05,
      "loss": 1.7914,
      "step": 3440
    },
    {
      "epoch": 1.816853192890059,
      "grad_norm": 0.7538657188415527,
      "learning_rate": 8.03287475433268e-05,
      "loss": 1.7269,
      "step": 3450
    },
    {
      "epoch": 1.8221198156682028,
      "grad_norm": 0.8729020357131958,
      "learning_rate": 7.997141325710202e-05,
      "loss": 1.6155,
      "step": 3460
    },
    {
      "epoch": 1.8273864384463463,
      "grad_norm": 0.6958781480789185,
      "learning_rate": 7.961407897087726e-05,
      "loss": 1.6838,
      "step": 3470
    },
    {
      "epoch": 1.8326530612244898,
      "grad_norm": 0.792233407497406,
      "learning_rate": 7.92567446846525e-05,
      "loss": 1.6714,
      "step": 3480
    },
    {
      "epoch": 1.8379196840026333,
      "grad_norm": 0.7291532754898071,
      "learning_rate": 7.889941039842773e-05,
      "loss": 1.6749,
      "step": 3490
    },
    {
      "epoch": 1.8431863067807768,
      "grad_norm": 0.7384997010231018,
      "learning_rate": 7.854207611220296e-05,
      "loss": 1.7804,
      "step": 3500
    },
    {
      "epoch": 1.8431863067807768,
      "eval_loss": 1.7604161500930786,
      "eval_runtime": 2088.2717,
      "eval_samples_per_second": 0.808,
      "eval_steps_per_second": 0.808,
      "step": 3500
    },
    {
      "epoch": 1.8484529295589205,
      "grad_norm": 0.7214270830154419,
      "learning_rate": 7.81847418259782e-05,
      "loss": 1.6612,
      "step": 3510
    },
    {
      "epoch": 1.853719552337064,
      "grad_norm": 0.7957137227058411,
      "learning_rate": 7.782740753975343e-05,
      "loss": 1.7123,
      "step": 3520
    },
    {
      "epoch": 1.8589861751152075,
      "grad_norm": 0.7388216257095337,
      "learning_rate": 7.747007325352868e-05,
      "loss": 1.7437,
      "step": 3530
    },
    {
      "epoch": 1.864252797893351,
      "grad_norm": 0.7733933329582214,
      "learning_rate": 7.711273896730392e-05,
      "loss": 1.7346,
      "step": 3540
    },
    {
      "epoch": 1.8695194206714945,
      "grad_norm": 0.7819935083389282,
      "learning_rate": 7.675540468107915e-05,
      "loss": 1.6807,
      "step": 3550
    },
    {
      "epoch": 1.874786043449638,
      "grad_norm": 0.7634021043777466,
      "learning_rate": 7.63980703948544e-05,
      "loss": 1.6542,
      "step": 3560
    },
    {
      "epoch": 1.8800526662277814,
      "grad_norm": 0.8159233331680298,
      "learning_rate": 7.604073610862963e-05,
      "loss": 1.6214,
      "step": 3570
    },
    {
      "epoch": 1.885319289005925,
      "grad_norm": 0.7680573463439941,
      "learning_rate": 7.568340182240486e-05,
      "loss": 1.7312,
      "step": 3580
    },
    {
      "epoch": 1.8905859117840684,
      "grad_norm": 0.7099646925926208,
      "learning_rate": 7.53260675361801e-05,
      "loss": 1.7368,
      "step": 3590
    },
    {
      "epoch": 1.895852534562212,
      "grad_norm": 0.8169664144515991,
      "learning_rate": 7.496873324995533e-05,
      "loss": 1.7823,
      "step": 3600
    },
    {
      "epoch": 1.9011191573403554,
      "grad_norm": 0.8348894715309143,
      "learning_rate": 7.461139896373058e-05,
      "loss": 1.7277,
      "step": 3610
    },
    {
      "epoch": 1.906385780118499,
      "grad_norm": 0.7611924409866333,
      "learning_rate": 7.425406467750581e-05,
      "loss": 1.657,
      "step": 3620
    },
    {
      "epoch": 1.9116524028966424,
      "grad_norm": 0.7578120827674866,
      "learning_rate": 7.389673039128104e-05,
      "loss": 1.725,
      "step": 3630
    },
    {
      "epoch": 1.916919025674786,
      "grad_norm": 0.7626579403877258,
      "learning_rate": 7.353939610505628e-05,
      "loss": 1.6948,
      "step": 3640
    },
    {
      "epoch": 1.9221856484529296,
      "grad_norm": 0.8038473725318909,
      "learning_rate": 7.318206181883151e-05,
      "loss": 1.6536,
      "step": 3650
    },
    {
      "epoch": 1.927452271231073,
      "grad_norm": 0.9015818238258362,
      "learning_rate": 7.282472753260676e-05,
      "loss": 1.6376,
      "step": 3660
    },
    {
      "epoch": 1.9327188940092166,
      "grad_norm": 0.8131605982780457,
      "learning_rate": 7.2467393246382e-05,
      "loss": 1.6323,
      "step": 3670
    },
    {
      "epoch": 1.93798551678736,
      "grad_norm": 0.7595666646957397,
      "learning_rate": 7.211005896015723e-05,
      "loss": 1.6443,
      "step": 3680
    },
    {
      "epoch": 1.9432521395655036,
      "grad_norm": 0.7437530159950256,
      "learning_rate": 7.175272467393247e-05,
      "loss": 1.6395,
      "step": 3690
    },
    {
      "epoch": 1.9485187623436473,
      "grad_norm": 0.7451024055480957,
      "learning_rate": 7.13953903877077e-05,
      "loss": 1.6741,
      "step": 3700
    },
    {
      "epoch": 1.9537853851217908,
      "grad_norm": 0.8429314494132996,
      "learning_rate": 7.103805610148294e-05,
      "loss": 1.6677,
      "step": 3710
    },
    {
      "epoch": 1.9590520078999343,
      "grad_norm": 0.790152370929718,
      "learning_rate": 7.068072181525818e-05,
      "loss": 1.6767,
      "step": 3720
    },
    {
      "epoch": 1.9643186306780778,
      "grad_norm": 0.7574294209480286,
      "learning_rate": 7.032338752903341e-05,
      "loss": 1.6781,
      "step": 3730
    },
    {
      "epoch": 1.9695852534562213,
      "grad_norm": 0.8111832737922668,
      "learning_rate": 6.996605324280865e-05,
      "loss": 1.7011,
      "step": 3740
    },
    {
      "epoch": 1.9748518762343648,
      "grad_norm": 0.7693150639533997,
      "learning_rate": 6.960871895658388e-05,
      "loss": 1.7395,
      "step": 3750
    },
    {
      "epoch": 1.9801184990125082,
      "grad_norm": 0.8176859021186829,
      "learning_rate": 6.925138467035912e-05,
      "loss": 1.6356,
      "step": 3760
    },
    {
      "epoch": 1.9853851217906517,
      "grad_norm": 0.7754336595535278,
      "learning_rate": 6.889405038413436e-05,
      "loss": 1.7611,
      "step": 3770
    },
    {
      "epoch": 1.9906517445687952,
      "grad_norm": 0.8608595132827759,
      "learning_rate": 6.853671609790959e-05,
      "loss": 1.7095,
      "step": 3780
    },
    {
      "epoch": 1.9959183673469387,
      "grad_norm": 0.7913508415222168,
      "learning_rate": 6.817938181168483e-05,
      "loss": 1.6764,
      "step": 3790
    },
    {
      "epoch": 2.001053324555629,
      "grad_norm": 0.7121990919113159,
      "learning_rate": 6.782204752546008e-05,
      "loss": 1.6527,
      "step": 3800
    },
    {
      "epoch": 2.0063199473337723,
      "grad_norm": 0.8046270608901978,
      "learning_rate": 6.746471323923531e-05,
      "loss": 1.6663,
      "step": 3810
    },
    {
      "epoch": 2.0115865701119158,
      "grad_norm": 0.7880458235740662,
      "learning_rate": 6.710737895301055e-05,
      "loss": 1.659,
      "step": 3820
    },
    {
      "epoch": 2.0168531928900593,
      "grad_norm": 0.9052935242652893,
      "learning_rate": 6.675004466678578e-05,
      "loss": 1.5669,
      "step": 3830
    },
    {
      "epoch": 2.0221198156682028,
      "grad_norm": 0.8260520100593567,
      "learning_rate": 6.639271038056101e-05,
      "loss": 1.6298,
      "step": 3840
    },
    {
      "epoch": 2.0273864384463463,
      "grad_norm": 0.8515588045120239,
      "learning_rate": 6.603537609433626e-05,
      "loss": 1.5736,
      "step": 3850
    },
    {
      "epoch": 2.0326530612244897,
      "grad_norm": 0.8033525943756104,
      "learning_rate": 6.567804180811149e-05,
      "loss": 1.5927,
      "step": 3860
    },
    {
      "epoch": 2.0379196840026332,
      "grad_norm": 0.7712513208389282,
      "learning_rate": 6.532070752188673e-05,
      "loss": 1.5996,
      "step": 3870
    },
    {
      "epoch": 2.0431863067807767,
      "grad_norm": 0.8318357467651367,
      "learning_rate": 6.496337323566196e-05,
      "loss": 1.5074,
      "step": 3880
    },
    {
      "epoch": 2.04845292955892,
      "grad_norm": 0.9031254649162292,
      "learning_rate": 6.460603894943719e-05,
      "loss": 1.6287,
      "step": 3890
    },
    {
      "epoch": 2.0537195523370637,
      "grad_norm": 0.8412225246429443,
      "learning_rate": 6.424870466321244e-05,
      "loss": 1.6048,
      "step": 3900
    },
    {
      "epoch": 2.058986175115207,
      "grad_norm": 0.905777633190155,
      "learning_rate": 6.389137037698768e-05,
      "loss": 1.5304,
      "step": 3910
    },
    {
      "epoch": 2.0642527978933507,
      "grad_norm": 0.8005120754241943,
      "learning_rate": 6.353403609076291e-05,
      "loss": 1.5396,
      "step": 3920
    },
    {
      "epoch": 2.069519420671494,
      "grad_norm": 0.8390421867370605,
      "learning_rate": 6.317670180453816e-05,
      "loss": 1.5791,
      "step": 3930
    },
    {
      "epoch": 2.074786043449638,
      "grad_norm": 0.8638313412666321,
      "learning_rate": 6.281936751831339e-05,
      "loss": 1.6034,
      "step": 3940
    },
    {
      "epoch": 2.0800526662277816,
      "grad_norm": 0.7873278856277466,
      "learning_rate": 6.246203323208863e-05,
      "loss": 1.6062,
      "step": 3950
    },
    {
      "epoch": 2.085319289005925,
      "grad_norm": 0.8260806798934937,
      "learning_rate": 6.210469894586386e-05,
      "loss": 1.5306,
      "step": 3960
    },
    {
      "epoch": 2.0905859117840686,
      "grad_norm": 0.8556029796600342,
      "learning_rate": 6.174736465963909e-05,
      "loss": 1.5728,
      "step": 3970
    },
    {
      "epoch": 2.095852534562212,
      "grad_norm": 0.8555281758308411,
      "learning_rate": 6.139003037341433e-05,
      "loss": 1.5809,
      "step": 3980
    },
    {
      "epoch": 2.1011191573403556,
      "grad_norm": 0.8599023222923279,
      "learning_rate": 6.1032696087189565e-05,
      "loss": 1.5843,
      "step": 3990
    },
    {
      "epoch": 2.106385780118499,
      "grad_norm": 0.8917784094810486,
      "learning_rate": 6.06753618009648e-05,
      "loss": 1.5989,
      "step": 4000
    },
    {
      "epoch": 2.106385780118499,
      "eval_loss": 1.7676888704299927,
      "eval_runtime": 1977.3052,
      "eval_samples_per_second": 0.854,
      "eval_steps_per_second": 0.854,
      "step": 4000
    },
    {
      "epoch": 2.1116524028966426,
      "grad_norm": 0.9284866452217102,
      "learning_rate": 6.031802751474004e-05,
      "loss": 1.5569,
      "step": 4010
    },
    {
      "epoch": 2.116919025674786,
      "grad_norm": 0.8434573411941528,
      "learning_rate": 5.996069322851528e-05,
      "loss": 1.5475,
      "step": 4020
    },
    {
      "epoch": 2.1221856484529296,
      "grad_norm": 0.8070669770240784,
      "learning_rate": 5.960335894229051e-05,
      "loss": 1.7319,
      "step": 4030
    },
    {
      "epoch": 2.127452271231073,
      "grad_norm": 0.9064502716064453,
      "learning_rate": 5.924602465606576e-05,
      "loss": 1.6412,
      "step": 4040
    },
    {
      "epoch": 2.1327188940092165,
      "grad_norm": 0.8301926255226135,
      "learning_rate": 5.8888690369840995e-05,
      "loss": 1.6021,
      "step": 4050
    },
    {
      "epoch": 2.13798551678736,
      "grad_norm": 0.8709795475006104,
      "learning_rate": 5.8531356083616226e-05,
      "loss": 1.5279,
      "step": 4060
    },
    {
      "epoch": 2.1432521395655035,
      "grad_norm": 0.8430463671684265,
      "learning_rate": 5.817402179739146e-05,
      "loss": 1.6311,
      "step": 4070
    },
    {
      "epoch": 2.148518762343647,
      "grad_norm": 0.8196474313735962,
      "learning_rate": 5.78166875111667e-05,
      "loss": 1.6151,
      "step": 4080
    },
    {
      "epoch": 2.1537853851217905,
      "grad_norm": 0.7925850749015808,
      "learning_rate": 5.745935322494194e-05,
      "loss": 1.5585,
      "step": 4090
    },
    {
      "epoch": 2.159052007899934,
      "grad_norm": 0.7950590252876282,
      "learning_rate": 5.7102018938717175e-05,
      "loss": 1.645,
      "step": 4100
    },
    {
      "epoch": 2.1643186306780775,
      "grad_norm": 0.8727664947509766,
      "learning_rate": 5.6744684652492405e-05,
      "loss": 1.7332,
      "step": 4110
    },
    {
      "epoch": 2.169585253456221,
      "grad_norm": 0.8788114190101624,
      "learning_rate": 5.638735036626764e-05,
      "loss": 1.696,
      "step": 4120
    },
    {
      "epoch": 2.1748518762343645,
      "grad_norm": 0.9276439547538757,
      "learning_rate": 5.603001608004288e-05,
      "loss": 1.5875,
      "step": 4130
    },
    {
      "epoch": 2.1801184990125084,
      "grad_norm": 0.822682797908783,
      "learning_rate": 5.567268179381812e-05,
      "loss": 1.5407,
      "step": 4140
    },
    {
      "epoch": 2.185385121790652,
      "grad_norm": 0.9170989394187927,
      "learning_rate": 5.5315347507593354e-05,
      "loss": 1.5981,
      "step": 4150
    },
    {
      "epoch": 2.1906517445687954,
      "grad_norm": 0.851089358329773,
      "learning_rate": 5.49580132213686e-05,
      "loss": 1.6,
      "step": 4160
    },
    {
      "epoch": 2.195918367346939,
      "grad_norm": 0.902912437915802,
      "learning_rate": 5.4600678935143836e-05,
      "loss": 1.613,
      "step": 4170
    },
    {
      "epoch": 2.2011849901250824,
      "grad_norm": 0.8360426425933838,
      "learning_rate": 5.424334464891907e-05,
      "loss": 1.5936,
      "step": 4180
    },
    {
      "epoch": 2.206451612903226,
      "grad_norm": 0.8562801480293274,
      "learning_rate": 5.38860103626943e-05,
      "loss": 1.5826,
      "step": 4190
    },
    {
      "epoch": 2.2117182356813694,
      "grad_norm": 0.8817809224128723,
      "learning_rate": 5.352867607646954e-05,
      "loss": 1.5929,
      "step": 4200
    },
    {
      "epoch": 2.216984858459513,
      "grad_norm": 0.9505837559700012,
      "learning_rate": 5.317134179024478e-05,
      "loss": 1.5802,
      "step": 4210
    },
    {
      "epoch": 2.2222514812376564,
      "grad_norm": 0.9158343076705933,
      "learning_rate": 5.2814007504020015e-05,
      "loss": 1.5821,
      "step": 4220
    },
    {
      "epoch": 2.2275181040158,
      "grad_norm": 0.8674532771110535,
      "learning_rate": 5.245667321779525e-05,
      "loss": 1.5615,
      "step": 4230
    },
    {
      "epoch": 2.2327847267939434,
      "grad_norm": 0.8162676095962524,
      "learning_rate": 5.209933893157048e-05,
      "loss": 1.5572,
      "step": 4240
    },
    {
      "epoch": 2.238051349572087,
      "grad_norm": 0.8892770409584045,
      "learning_rate": 5.174200464534572e-05,
      "loss": 1.5938,
      "step": 4250
    },
    {
      "epoch": 2.2433179723502303,
      "grad_norm": 0.8849489688873291,
      "learning_rate": 5.138467035912096e-05,
      "loss": 1.5786,
      "step": 4260
    },
    {
      "epoch": 2.248584595128374,
      "grad_norm": 0.8824392557144165,
      "learning_rate": 5.1027336072896194e-05,
      "loss": 1.6043,
      "step": 4270
    },
    {
      "epoch": 2.2538512179065173,
      "grad_norm": 0.8386142253875732,
      "learning_rate": 5.0670001786671425e-05,
      "loss": 1.593,
      "step": 4280
    },
    {
      "epoch": 2.259117840684661,
      "grad_norm": 0.9126545190811157,
      "learning_rate": 5.0312667500446676e-05,
      "loss": 1.7044,
      "step": 4290
    },
    {
      "epoch": 2.2643844634628043,
      "grad_norm": 0.8334399461746216,
      "learning_rate": 4.9955333214221906e-05,
      "loss": 1.6314,
      "step": 4300
    },
    {
      "epoch": 2.269651086240948,
      "grad_norm": 0.8445755243301392,
      "learning_rate": 4.9597998927997143e-05,
      "loss": 1.5196,
      "step": 4310
    },
    {
      "epoch": 2.2749177090190917,
      "grad_norm": 0.9420812129974365,
      "learning_rate": 4.924066464177238e-05,
      "loss": 1.5977,
      "step": 4320
    },
    {
      "epoch": 2.2801843317972352,
      "grad_norm": 0.8875234127044678,
      "learning_rate": 4.888333035554762e-05,
      "loss": 1.5929,
      "step": 4330
    },
    {
      "epoch": 2.2854509545753787,
      "grad_norm": 0.9258802533149719,
      "learning_rate": 4.8525996069322855e-05,
      "loss": 1.6032,
      "step": 4340
    },
    {
      "epoch": 2.290717577353522,
      "grad_norm": 0.8369778990745544,
      "learning_rate": 4.816866178309809e-05,
      "loss": 1.5314,
      "step": 4350
    },
    {
      "epoch": 2.2959842001316657,
      "grad_norm": 0.9020923376083374,
      "learning_rate": 4.781132749687332e-05,
      "loss": 1.5865,
      "step": 4360
    },
    {
      "epoch": 2.301250822909809,
      "grad_norm": 0.8916406035423279,
      "learning_rate": 4.745399321064856e-05,
      "loss": 1.7168,
      "step": 4370
    },
    {
      "epoch": 2.3065174456879527,
      "grad_norm": 0.8179642558097839,
      "learning_rate": 4.7096658924423804e-05,
      "loss": 1.6017,
      "step": 4380
    },
    {
      "epoch": 2.311784068466096,
      "grad_norm": 0.8550540804862976,
      "learning_rate": 4.673932463819904e-05,
      "loss": 1.5693,
      "step": 4390
    },
    {
      "epoch": 2.3170506912442397,
      "grad_norm": 0.9279409050941467,
      "learning_rate": 4.638199035197427e-05,
      "loss": 1.6459,
      "step": 4400
    },
    {
      "epoch": 2.322317314022383,
      "grad_norm": 0.9227657914161682,
      "learning_rate": 4.602465606574951e-05,
      "loss": 1.5699,
      "step": 4410
    },
    {
      "epoch": 2.3275839368005267,
      "grad_norm": 0.825989305973053,
      "learning_rate": 4.5667321779524746e-05,
      "loss": 1.62,
      "step": 4420
    },
    {
      "epoch": 2.33285055957867,
      "grad_norm": 0.8304446935653687,
      "learning_rate": 4.5309987493299984e-05,
      "loss": 1.5082,
      "step": 4430
    },
    {
      "epoch": 2.3381171823568137,
      "grad_norm": 0.8860904574394226,
      "learning_rate": 4.495265320707522e-05,
      "loss": 1.5816,
      "step": 4440
    },
    {
      "epoch": 2.343383805134957,
      "grad_norm": 0.9233811497688293,
      "learning_rate": 4.459531892085046e-05,
      "loss": 1.5636,
      "step": 4450
    },
    {
      "epoch": 2.3486504279131006,
      "grad_norm": 0.8352006077766418,
      "learning_rate": 4.4237984634625695e-05,
      "loss": 1.6087,
      "step": 4460
    },
    {
      "epoch": 2.353917050691244,
      "grad_norm": 0.8998164534568787,
      "learning_rate": 4.388065034840093e-05,
      "loss": 1.5889,
      "step": 4470
    },
    {
      "epoch": 2.3591836734693876,
      "grad_norm": 1.0011513233184814,
      "learning_rate": 4.352331606217617e-05,
      "loss": 1.4867,
      "step": 4480
    },
    {
      "epoch": 2.364450296247531,
      "grad_norm": 0.885601818561554,
      "learning_rate": 4.31659817759514e-05,
      "loss": 1.5099,
      "step": 4490
    },
    {
      "epoch": 2.3697169190256746,
      "grad_norm": 0.9775383472442627,
      "learning_rate": 4.280864748972664e-05,
      "loss": 1.5468,
      "step": 4500
    },
    {
      "epoch": 2.3697169190256746,
      "eval_loss": 1.7650710344314575,
      "eval_runtime": 1860.4273,
      "eval_samples_per_second": 0.907,
      "eval_steps_per_second": 0.907,
      "step": 4500
    },
    {
      "epoch": 2.374983541803818,
      "grad_norm": 0.9137322306632996,
      "learning_rate": 4.245131320350188e-05,
      "loss": 1.5534,
      "step": 4510
    },
    {
      "epoch": 2.3802501645819616,
      "grad_norm": 0.9367275834083557,
      "learning_rate": 4.209397891727712e-05,
      "loss": 1.6452,
      "step": 4520
    },
    {
      "epoch": 2.385516787360105,
      "grad_norm": 0.8319429755210876,
      "learning_rate": 4.173664463105235e-05,
      "loss": 1.6221,
      "step": 4530
    },
    {
      "epoch": 2.390783410138249,
      "grad_norm": 0.8511477112770081,
      "learning_rate": 4.1379310344827587e-05,
      "loss": 1.6355,
      "step": 4540
    },
    {
      "epoch": 2.3960500329163925,
      "grad_norm": 0.8601189851760864,
      "learning_rate": 4.1021976058602824e-05,
      "loss": 1.6129,
      "step": 4550
    },
    {
      "epoch": 2.401316655694536,
      "grad_norm": 0.8665868639945984,
      "learning_rate": 4.066464177237806e-05,
      "loss": 1.6959,
      "step": 4560
    },
    {
      "epoch": 2.4065832784726795,
      "grad_norm": 0.9494168758392334,
      "learning_rate": 4.03073074861533e-05,
      "loss": 1.6128,
      "step": 4570
    },
    {
      "epoch": 2.411849901250823,
      "grad_norm": 0.8704026341438293,
      "learning_rate": 3.9949973199928536e-05,
      "loss": 1.5504,
      "step": 4580
    },
    {
      "epoch": 2.4171165240289665,
      "grad_norm": 0.8754653930664062,
      "learning_rate": 3.959263891370377e-05,
      "loss": 1.4942,
      "step": 4590
    },
    {
      "epoch": 2.42238314680711,
      "grad_norm": 0.8762432932853699,
      "learning_rate": 3.923530462747901e-05,
      "loss": 1.5423,
      "step": 4600
    },
    {
      "epoch": 2.4276497695852535,
      "grad_norm": 0.939985990524292,
      "learning_rate": 3.887797034125425e-05,
      "loss": 1.5753,
      "step": 4610
    },
    {
      "epoch": 2.432916392363397,
      "grad_norm": 0.8544850945472717,
      "learning_rate": 3.852063605502948e-05,
      "loss": 1.6518,
      "step": 4620
    },
    {
      "epoch": 2.4381830151415405,
      "grad_norm": 0.9352607727050781,
      "learning_rate": 3.816330176880472e-05,
      "loss": 1.5317,
      "step": 4630
    },
    {
      "epoch": 2.443449637919684,
      "grad_norm": 0.8941733837127686,
      "learning_rate": 3.780596748257996e-05,
      "loss": 1.607,
      "step": 4640
    },
    {
      "epoch": 2.4487162606978274,
      "grad_norm": 0.855034589767456,
      "learning_rate": 3.744863319635519e-05,
      "loss": 1.5908,
      "step": 4650
    },
    {
      "epoch": 2.453982883475971,
      "grad_norm": 0.8693886399269104,
      "learning_rate": 3.709129891013043e-05,
      "loss": 1.6674,
      "step": 4660
    },
    {
      "epoch": 2.4592495062541144,
      "grad_norm": 0.836715817451477,
      "learning_rate": 3.6733964623905664e-05,
      "loss": 1.5272,
      "step": 4670
    },
    {
      "epoch": 2.464516129032258,
      "grad_norm": 0.9121700525283813,
      "learning_rate": 3.63766303376809e-05,
      "loss": 1.6389,
      "step": 4680
    },
    {
      "epoch": 2.4697827518104014,
      "grad_norm": 0.8818648457527161,
      "learning_rate": 3.601929605145614e-05,
      "loss": 1.5878,
      "step": 4690
    },
    {
      "epoch": 2.475049374588545,
      "grad_norm": 0.8658255338668823,
      "learning_rate": 3.5661961765231376e-05,
      "loss": 1.6165,
      "step": 4700
    },
    {
      "epoch": 2.480315997366689,
      "grad_norm": 0.9144027829170227,
      "learning_rate": 3.530462747900661e-05,
      "loss": 1.6178,
      "step": 4710
    },
    {
      "epoch": 2.4855826201448323,
      "grad_norm": 0.8756635785102844,
      "learning_rate": 3.494729319278185e-05,
      "loss": 1.6195,
      "step": 4720
    },
    {
      "epoch": 2.490849242922976,
      "grad_norm": 0.9023842215538025,
      "learning_rate": 3.458995890655709e-05,
      "loss": 1.6451,
      "step": 4730
    },
    {
      "epoch": 2.4961158657011193,
      "grad_norm": 0.8563377857208252,
      "learning_rate": 3.423262462033232e-05,
      "loss": 1.6069,
      "step": 4740
    },
    {
      "epoch": 2.501382488479263,
      "grad_norm": 0.857814371585846,
      "learning_rate": 3.3875290334107555e-05,
      "loss": 1.534,
      "step": 4750
    },
    {
      "epoch": 2.5066491112574063,
      "grad_norm": 0.9222397804260254,
      "learning_rate": 3.35179560478828e-05,
      "loss": 1.4909,
      "step": 4760
    },
    {
      "epoch": 2.51191573403555,
      "grad_norm": 0.870204746723175,
      "learning_rate": 3.3160621761658036e-05,
      "loss": 1.6627,
      "step": 4770
    },
    {
      "epoch": 2.5171823568136933,
      "grad_norm": 0.9122799634933472,
      "learning_rate": 3.280328747543327e-05,
      "loss": 1.5575,
      "step": 4780
    },
    {
      "epoch": 2.522448979591837,
      "grad_norm": 0.8926681280136108,
      "learning_rate": 3.2445953189208504e-05,
      "loss": 1.5842,
      "step": 4790
    },
    {
      "epoch": 2.5277156023699803,
      "grad_norm": 0.8344525098800659,
      "learning_rate": 3.208861890298374e-05,
      "loss": 1.5967,
      "step": 4800
    },
    {
      "epoch": 2.5329822251481238,
      "grad_norm": 0.9111979603767395,
      "learning_rate": 3.173128461675898e-05,
      "loss": 1.5588,
      "step": 4810
    },
    {
      "epoch": 2.5382488479262673,
      "grad_norm": 0.8867557048797607,
      "learning_rate": 3.1373950330534216e-05,
      "loss": 1.5973,
      "step": 4820
    },
    {
      "epoch": 2.5435154707044108,
      "grad_norm": 0.8460855484008789,
      "learning_rate": 3.101661604430945e-05,
      "loss": 1.6112,
      "step": 4830
    },
    {
      "epoch": 2.5487820934825542,
      "grad_norm": 0.894749104976654,
      "learning_rate": 3.065928175808469e-05,
      "loss": 1.564,
      "step": 4840
    },
    {
      "epoch": 2.5540487162606977,
      "grad_norm": 0.8443318009376526,
      "learning_rate": 3.0301947471859928e-05,
      "loss": 1.6378,
      "step": 4850
    },
    {
      "epoch": 2.5593153390388412,
      "grad_norm": 0.9015233516693115,
      "learning_rate": 2.994461318563516e-05,
      "loss": 1.6143,
      "step": 4860
    },
    {
      "epoch": 2.5645819618169847,
      "grad_norm": 0.8445819020271301,
      "learning_rate": 2.95872788994104e-05,
      "loss": 1.4683,
      "step": 4870
    },
    {
      "epoch": 2.569848584595128,
      "grad_norm": 0.873344361782074,
      "learning_rate": 2.922994461318564e-05,
      "loss": 1.572,
      "step": 4880
    },
    {
      "epoch": 2.5751152073732717,
      "grad_norm": 0.8657721281051636,
      "learning_rate": 2.8872610326960877e-05,
      "loss": 1.5763,
      "step": 4890
    },
    {
      "epoch": 2.580381830151415,
      "grad_norm": 0.895204484462738,
      "learning_rate": 2.851527604073611e-05,
      "loss": 1.5192,
      "step": 4900
    },
    {
      "epoch": 2.5856484529295587,
      "grad_norm": 0.8737152218818665,
      "learning_rate": 2.8157941754511348e-05,
      "loss": 1.5916,
      "step": 4910
    },
    {
      "epoch": 2.590915075707702,
      "grad_norm": 0.840100109577179,
      "learning_rate": 2.780060746828658e-05,
      "loss": 1.6028,
      "step": 4920
    },
    {
      "epoch": 2.5961816984858457,
      "grad_norm": 0.8778549432754517,
      "learning_rate": 2.744327318206182e-05,
      "loss": 1.5047,
      "step": 4930
    },
    {
      "epoch": 2.6014483212639896,
      "grad_norm": 0.904563307762146,
      "learning_rate": 2.7085938895837053e-05,
      "loss": 1.5914,
      "step": 4940
    },
    {
      "epoch": 2.606714944042133,
      "grad_norm": 0.8521222472190857,
      "learning_rate": 2.6728604609612297e-05,
      "loss": 1.5028,
      "step": 4950
    },
    {
      "epoch": 2.6119815668202766,
      "grad_norm": 0.9160414934158325,
      "learning_rate": 2.637127032338753e-05,
      "loss": 1.6261,
      "step": 4960
    },
    {
      "epoch": 2.61724818959842,
      "grad_norm": 0.8594067096710205,
      "learning_rate": 2.6013936037162768e-05,
      "loss": 1.5185,
      "step": 4970
    },
    {
      "epoch": 2.6225148123765636,
      "grad_norm": 0.9424293637275696,
      "learning_rate": 2.5656601750938e-05,
      "loss": 1.6465,
      "step": 4980
    },
    {
      "epoch": 2.627781435154707,
      "grad_norm": 0.8839238286018372,
      "learning_rate": 2.529926746471324e-05,
      "loss": 1.5798,
      "step": 4990
    },
    {
      "epoch": 2.6330480579328506,
      "grad_norm": 0.8784611821174622,
      "learning_rate": 2.4941933178488476e-05,
      "loss": 1.6262,
      "step": 5000
    },
    {
      "epoch": 2.6330480579328506,
      "eval_loss": 1.7617226839065552,
      "eval_runtime": 1897.6806,
      "eval_samples_per_second": 0.89,
      "eval_steps_per_second": 0.89,
      "step": 5000
    },
    {
      "epoch": 2.638314680710994,
      "grad_norm": 0.9332797527313232,
      "learning_rate": 2.4584598892263713e-05,
      "loss": 1.5703,
      "step": 5010
    },
    {
      "epoch": 2.6435813034891376,
      "grad_norm": 0.865289568901062,
      "learning_rate": 2.422726460603895e-05,
      "loss": 1.6222,
      "step": 5020
    },
    {
      "epoch": 2.648847926267281,
      "grad_norm": 0.8798305988311768,
      "learning_rate": 2.3869930319814188e-05,
      "loss": 1.6071,
      "step": 5030
    },
    {
      "epoch": 2.6541145490454245,
      "grad_norm": 0.9365794658660889,
      "learning_rate": 2.3512596033589425e-05,
      "loss": 1.6007,
      "step": 5040
    },
    {
      "epoch": 2.659381171823568,
      "grad_norm": 0.8994954228401184,
      "learning_rate": 2.315526174736466e-05,
      "loss": 1.5405,
      "step": 5050
    },
    {
      "epoch": 2.6646477946017115,
      "grad_norm": 0.9717227816581726,
      "learning_rate": 2.27979274611399e-05,
      "loss": 1.6669,
      "step": 5060
    },
    {
      "epoch": 2.669914417379855,
      "grad_norm": 0.8539732098579407,
      "learning_rate": 2.2440593174915133e-05,
      "loss": 1.5738,
      "step": 5070
    },
    {
      "epoch": 2.675181040157999,
      "grad_norm": 0.9548415541648865,
      "learning_rate": 2.208325888869037e-05,
      "loss": 1.643,
      "step": 5080
    },
    {
      "epoch": 2.6804476629361425,
      "grad_norm": 0.8917221426963806,
      "learning_rate": 2.1725924602465608e-05,
      "loss": 1.5504,
      "step": 5090
    },
    {
      "epoch": 2.685714285714286,
      "grad_norm": 0.9324446320533752,
      "learning_rate": 2.1368590316240845e-05,
      "loss": 1.6034,
      "step": 5100
    },
    {
      "epoch": 2.6909809084924294,
      "grad_norm": 0.8756060004234314,
      "learning_rate": 2.101125603001608e-05,
      "loss": 1.6715,
      "step": 5110
    },
    {
      "epoch": 2.696247531270573,
      "grad_norm": 0.879608154296875,
      "learning_rate": 2.065392174379132e-05,
      "loss": 1.5,
      "step": 5120
    },
    {
      "epoch": 2.7015141540487164,
      "grad_norm": 0.8506643772125244,
      "learning_rate": 2.0296587457566554e-05,
      "loss": 1.6514,
      "step": 5130
    },
    {
      "epoch": 2.70678077682686,
      "grad_norm": 0.8191288113594055,
      "learning_rate": 1.993925317134179e-05,
      "loss": 1.6657,
      "step": 5140
    },
    {
      "epoch": 2.7120473996050034,
      "grad_norm": 0.9187442660331726,
      "learning_rate": 1.9581918885117028e-05,
      "loss": 1.6072,
      "step": 5150
    },
    {
      "epoch": 2.717314022383147,
      "grad_norm": 0.88078773021698,
      "learning_rate": 1.9224584598892265e-05,
      "loss": 1.5306,
      "step": 5160
    },
    {
      "epoch": 2.7225806451612904,
      "grad_norm": 0.9224525094032288,
      "learning_rate": 1.88672503126675e-05,
      "loss": 1.6473,
      "step": 5170
    },
    {
      "epoch": 2.727847267939434,
      "grad_norm": 0.9067012071609497,
      "learning_rate": 1.850991602644274e-05,
      "loss": 1.5883,
      "step": 5180
    },
    {
      "epoch": 2.7331138907175774,
      "grad_norm": 0.860525906085968,
      "learning_rate": 1.8152581740217974e-05,
      "loss": 1.6102,
      "step": 5190
    },
    {
      "epoch": 2.738380513495721,
      "grad_norm": 0.8802415132522583,
      "learning_rate": 1.779524745399321e-05,
      "loss": 1.6381,
      "step": 5200
    },
    {
      "epoch": 2.7436471362738644,
      "grad_norm": 0.9391332864761353,
      "learning_rate": 1.7437913167768448e-05,
      "loss": 1.5828,
      "step": 5210
    },
    {
      "epoch": 2.748913759052008,
      "grad_norm": 0.9115069508552551,
      "learning_rate": 1.7080578881543685e-05,
      "loss": 1.6665,
      "step": 5220
    },
    {
      "epoch": 2.7541803818301513,
      "grad_norm": 0.88495272397995,
      "learning_rate": 1.6723244595318923e-05,
      "loss": 1.6631,
      "step": 5230
    },
    {
      "epoch": 2.759447004608295,
      "grad_norm": 0.9101977944374084,
      "learning_rate": 1.636591030909416e-05,
      "loss": 1.5721,
      "step": 5240
    },
    {
      "epoch": 2.7647136273864383,
      "grad_norm": 0.8895593285560608,
      "learning_rate": 1.6008576022869394e-05,
      "loss": 1.5982,
      "step": 5250
    },
    {
      "epoch": 2.769980250164582,
      "grad_norm": 0.8863037824630737,
      "learning_rate": 1.565124173664463e-05,
      "loss": 1.6731,
      "step": 5260
    },
    {
      "epoch": 2.7752468729427253,
      "grad_norm": 0.8671737313270569,
      "learning_rate": 1.5293907450419868e-05,
      "loss": 1.6981,
      "step": 5270
    },
    {
      "epoch": 2.780513495720869,
      "grad_norm": 0.9136113524436951,
      "learning_rate": 1.4936573164195105e-05,
      "loss": 1.5837,
      "step": 5280
    },
    {
      "epoch": 2.7857801184990123,
      "grad_norm": 0.9174538850784302,
      "learning_rate": 1.4579238877970341e-05,
      "loss": 1.6946,
      "step": 5290
    },
    {
      "epoch": 2.791046741277156,
      "grad_norm": 0.9553307890892029,
      "learning_rate": 1.4221904591745577e-05,
      "loss": 1.579,
      "step": 5300
    },
    {
      "epoch": 2.7963133640552993,
      "grad_norm": 0.9689258337020874,
      "learning_rate": 1.3864570305520816e-05,
      "loss": 1.6001,
      "step": 5310
    },
    {
      "epoch": 2.801579986833443,
      "grad_norm": 0.9052961468696594,
      "learning_rate": 1.3507236019296051e-05,
      "loss": 1.6337,
      "step": 5320
    },
    {
      "epoch": 2.8068466096115867,
      "grad_norm": 1.0103005170822144,
      "learning_rate": 1.3149901733071288e-05,
      "loss": 1.5432,
      "step": 5330
    },
    {
      "epoch": 2.81211323238973,
      "grad_norm": 0.9238522052764893,
      "learning_rate": 1.2792567446846526e-05,
      "loss": 1.5732,
      "step": 5340
    },
    {
      "epoch": 2.8173798551678737,
      "grad_norm": 0.9226769208908081,
      "learning_rate": 1.2435233160621763e-05,
      "loss": 1.6116,
      "step": 5350
    },
    {
      "epoch": 2.822646477946017,
      "grad_norm": 0.8547074794769287,
      "learning_rate": 1.2077898874397e-05,
      "loss": 1.6259,
      "step": 5360
    },
    {
      "epoch": 2.8279131007241607,
      "grad_norm": 0.8599969148635864,
      "learning_rate": 1.1720564588172236e-05,
      "loss": 1.4982,
      "step": 5370
    },
    {
      "epoch": 2.833179723502304,
      "grad_norm": 0.9194432497024536,
      "learning_rate": 1.1363230301947473e-05,
      "loss": 1.6252,
      "step": 5380
    },
    {
      "epoch": 2.8384463462804477,
      "grad_norm": 0.8877129554748535,
      "learning_rate": 1.100589601572271e-05,
      "loss": 1.5947,
      "step": 5390
    },
    {
      "epoch": 2.843712969058591,
      "grad_norm": 0.8904669284820557,
      "learning_rate": 1.0648561729497946e-05,
      "loss": 1.5688,
      "step": 5400
    },
    {
      "epoch": 2.8489795918367347,
      "grad_norm": 0.8910465836524963,
      "learning_rate": 1.0291227443273183e-05,
      "loss": 1.6166,
      "step": 5410
    },
    {
      "epoch": 2.854246214614878,
      "grad_norm": 0.9230101108551025,
      "learning_rate": 9.93389315704842e-06,
      "loss": 1.481,
      "step": 5420
    },
    {
      "epoch": 2.8595128373930216,
      "grad_norm": 0.87525475025177,
      "learning_rate": 9.576558870823656e-06,
      "loss": 1.5346,
      "step": 5430
    },
    {
      "epoch": 2.864779460171165,
      "grad_norm": 0.8926069736480713,
      "learning_rate": 9.219224584598893e-06,
      "loss": 1.5632,
      "step": 5440
    },
    {
      "epoch": 2.8700460829493086,
      "grad_norm": 0.9119460582733154,
      "learning_rate": 8.86189029837413e-06,
      "loss": 1.699,
      "step": 5450
    },
    {
      "epoch": 2.875312705727452,
      "grad_norm": 0.8521437644958496,
      "learning_rate": 8.504556012149366e-06,
      "loss": 1.6149,
      "step": 5460
    },
    {
      "epoch": 2.880579328505596,
      "grad_norm": 0.8682225346565247,
      "learning_rate": 8.147221725924603e-06,
      "loss": 1.5624,
      "step": 5470
    },
    {
      "epoch": 2.8858459512837396,
      "grad_norm": 0.8336487412452698,
      "learning_rate": 7.789887439699839e-06,
      "loss": 1.6814,
      "step": 5480
    },
    {
      "epoch": 2.891112574061883,
      "grad_norm": 0.9421902894973755,
      "learning_rate": 7.432553153475076e-06,
      "loss": 1.5724,
      "step": 5490
    },
    {
      "epoch": 2.8963791968400265,
      "grad_norm": 0.8765360713005066,
      "learning_rate": 7.075218867250313e-06,
      "loss": 1.6045,
      "step": 5500
    },
    {
      "epoch": 2.8963791968400265,
      "eval_loss": 1.7588139772415161,
      "eval_runtime": 1943.1645,
      "eval_samples_per_second": 0.869,
      "eval_steps_per_second": 0.869,
      "step": 5500
    },
    {
      "epoch": 2.90164581961817,
      "grad_norm": 0.8605120778083801,
      "learning_rate": 6.7178845810255494e-06,
      "loss": 1.6176,
      "step": 5510
    },
    {
      "epoch": 2.9069124423963135,
      "grad_norm": 0.9245268702507019,
      "learning_rate": 6.360550294800787e-06,
      "loss": 1.5752,
      "step": 5520
    },
    {
      "epoch": 2.912179065174457,
      "grad_norm": 0.9760609269142151,
      "learning_rate": 6.003216008576023e-06,
      "loss": 1.5822,
      "step": 5530
    },
    {
      "epoch": 2.9174456879526005,
      "grad_norm": 0.8613636493682861,
      "learning_rate": 5.64588172235126e-06,
      "loss": 1.5595,
      "step": 5540
    },
    {
      "epoch": 2.922712310730744,
      "grad_norm": 0.8654859662055969,
      "learning_rate": 5.288547436126497e-06,
      "loss": 1.4928,
      "step": 5550
    },
    {
      "epoch": 2.9279789335088875,
      "grad_norm": 0.9045332670211792,
      "learning_rate": 4.931213149901733e-06,
      "loss": 1.6744,
      "step": 5560
    },
    {
      "epoch": 2.933245556287031,
      "grad_norm": 0.887523353099823,
      "learning_rate": 4.5738788636769695e-06,
      "loss": 1.5603,
      "step": 5570
    },
    {
      "epoch": 2.9385121790651745,
      "grad_norm": 0.908867359161377,
      "learning_rate": 4.216544577452207e-06,
      "loss": 1.6496,
      "step": 5580
    },
    {
      "epoch": 2.943778801843318,
      "grad_norm": 0.9479954242706299,
      "learning_rate": 3.859210291227443e-06,
      "loss": 1.5639,
      "step": 5590
    },
    {
      "epoch": 2.9490454246214615,
      "grad_norm": 0.910720705986023,
      "learning_rate": 3.50187600500268e-06,
      "loss": 1.5745,
      "step": 5600
    },
    {
      "epoch": 2.954312047399605,
      "grad_norm": 0.8970969319343567,
      "learning_rate": 3.1445417187779172e-06,
      "loss": 1.5407,
      "step": 5610
    },
    {
      "epoch": 2.9595786701777484,
      "grad_norm": 0.8968970775604248,
      "learning_rate": 2.7872074325531536e-06,
      "loss": 1.5884,
      "step": 5620
    },
    {
      "epoch": 2.964845292955892,
      "grad_norm": 0.9638487100601196,
      "learning_rate": 2.4298731463283905e-06,
      "loss": 1.5922,
      "step": 5630
    },
    {
      "epoch": 2.9701119157340354,
      "grad_norm": 0.895858645439148,
      "learning_rate": 2.0725388601036273e-06,
      "loss": 1.612,
      "step": 5640
    },
    {
      "epoch": 2.975378538512179,
      "grad_norm": 0.8754726052284241,
      "learning_rate": 1.7152045738788637e-06,
      "loss": 1.5749,
      "step": 5650
    },
    {
      "epoch": 2.9806451612903224,
      "grad_norm": 0.8650164008140564,
      "learning_rate": 1.3578702876541005e-06,
      "loss": 1.6226,
      "step": 5660
    },
    {
      "epoch": 2.985911784068466,
      "grad_norm": 0.9245826005935669,
      "learning_rate": 1.0005360014293371e-06,
      "loss": 1.6353,
      "step": 5670
    },
    {
      "epoch": 2.9911784068466094,
      "grad_norm": 0.9029963612556458,
      "learning_rate": 6.432017152045739e-07,
      "loss": 1.5427,
      "step": 5680
    },
    {
      "epoch": 2.996445029624753,
      "grad_norm": 0.8227625489234924,
      "learning_rate": 2.858674289798106e-07,
      "loss": 1.4699,
      "step": 5690
    }
  ],
  "logging_steps": 10,
  "max_steps": 5697,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.9906156520111555e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
